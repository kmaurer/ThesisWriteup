% Template file for a standard thesis
\documentclass[11pt]{isuthesis}
%---------------------------------------------------
% \usepackage{color}
% \usepackage[dvipsnames,svgnames,table]{xcolor}
\usepackage{wrapfig,float}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{url}
\usepackage{ulem}
\usepackage[section]{placeins}
\usepackage{multirow}
\usepackage{bbm}
\usepackage{rotating}
\usepackage[referable]{threeparttablex}
\usepackage{footnote} % table footnotes
\usepackage{multicol}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{fullpage}
\usepackage{graphicx,float,wrapfig,subfig,tabularx,ulem}
\graphicspath{{Figure/}}
%\usepackage{csquotes}
\usepackage{color}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{lipsum}
\usepackage{url}
\usepackage{bbm}
\usepackage[titletoc]{appendix}



\newcommand{\distas}[1]{\mathbin{\overset{#1}{\sim}}}%
\newcommand{\Cov}{\text{Cov}}%

\newcommand{\hh}[1]{{\color{ForestGreen} #1}}
\newcommand{\km}[1]{{\color{red} #1}}
\newcommand{\ktm}[1]{{\color{red} #1}}
\newcommand{\V}[1]{\text{Var}\left(#1\right)}

%---------------------------------------------------

% Standard, old-style thesis
\usepackage{isutraditional}   
\chaptertitle

% Old-style, thesis numbering down to subsubsection
\alternate
\usepackage{rotating}

% Bibliography without numbers or labels
\usepackage{natbib}
\bibliographystyle{asa}

%Optional Package to add PDF bookmarks and hypertext links
\usepackage[pdftex,hypertexnames=false,linktocpage=true]{hyperref}
\hypersetup{colorlinks=true,linkcolor=blue,anchorcolor=blue,citecolor=blue,filecolor=blue,urlcolor=blue,bookmarksnumbered=true,pdfview=FitB}

% %------------------------------------------------------

% Force TOC/LOT/LOF to add a pagebreak if there isn't room 
% for at least N lines on the current page
\newcommand\chaptocbreak{
	\addtocontents{toc}{\protect\needspace{4\baselineskip}}
	\addtocontents{lof}{\protect\needspace{2\baselineskip}}
	\addtocontents{lot}{\protect\needspace{2\baselineskip}}
}

% %------------------------------------------------------

\newtheorem{theorem}{Theorem}[section]
\newtheorem{algorithm}[theorem]{Algorithm}

%---------------------------------------------------

\begin{document}
\DeclareGraphicsExtensions{.jpg,.pdf,.mps,.png}
%-------------------------------------------------------------------------------
\include{Preface/titlepage}
%-------------------------------------------------------------------------------
% Optional thesis dedication
% \include{Preface/dedication}
%-------------------------------------------------------------------------------
% Table of Contents, List of Tables and List of Figures
\pdfbookmark[1]{TABLE OF CONTENTS}{table}
\tableofcontents
\addtocontents{toc}{\def\protect\@chapapp{}} \cleardoublepage \phantomsection
\addcontentsline{toc}{chapter}{LIST OF TABLES}
\listoftables
\cleardoublepage \phantomsection \addcontentsline{toc}{chapter}{LIST OF FIGURES}
\listoffigures
% Comment out the next line if NOT using chaptertitle
\addtocontents{toc}{\def\protect\@chapapp{CHAPTER\ }}
%-------------------------------------------------------------------------------
%Optional Dedication
\cleardoublepage \phantomsection
\include{Preface/dedication}
%-------------------------------------------------------------------------------
%Optional Acknowledgements
\cleardoublepage \phantomsection
\include{Preface/acknowl}
%-------------------------------------------------------------------------------
%Optional thesis abstract
% \cleardoublepage \phantomsection
% \include{Preface/abstract}
%-------------------------------------------------------------------------------
\newpage
\pagenumbering{arabic}
%-------------------------------------------------------------------------------


\chapter{LITERATURE REVIEW}\label{litreview}

\section{Introduction}

This dissertation is a composite of research preformed in the fields of statistics education and statistical graphics. The three body chapters stand as the pillars of this work; tied together by the common theme of overcoming challenges and grasping opportunities that are posed by emerging technologies and prodigious data sources. In this first chapter a review of the literature is conducted to lay the foundation upon which the work of the following chapters is built. 

We begin by investigating literature on the technological history of statistical education and a review of current uses of technological tools in the undergraduate statistics classroom. Development and application of educational technology in similar STEM disciplines are also explored to identify general pedagogical and design principles for effective implementation of technology in statistics curricula. As a note, Section~\ref{EdTech} is intended to be submitted for publication to the Journal of Statistics Education (JSE) as a stand alone review of literature on technology in statistics education.

The remaining sections of the literature review then investigate work from fields pertinent to the individual body chapters of this dissertation. Chapter 2 is an educational experiment comparing the learning outcomes from simulation-based and traditional statistical inference curricula. Literature to support this study come from the subjects of simulation-based inference curriculum development, learning assessment, comparative educational studies and experimentalism in education. Chapter 3 studies the development of a \texttt{shiny} \citep{shiny} application to connect students to large data; thus literature on developing and evaluating educational technology, as well as existing tools for statistics education and data science, are reviewed. Chapter 4 is tangential to the work with large data found in the \texttt{shiny} application, however it contributes to research on binned scatterplots as a graphical tool for visualizing large data. Pertinent literature for this research include works on binning strategies, statistical graphics, and perceptual psychology. Each body chapter in this dissertation is intended be submitted for publication individually; therefore, the works discussed in this comprehensive literature review will also be found cited throughout the respective chapters.

\section{Technology in Statistics Education}
\label{EdTech}

%The jumping off point for this dissertation is the theme of technology in statistics education. To provide a coherent overview of this broad subject 
We will begin with a review of the evolution of technology in the statistics education reformation movement. We then discuss specific implementations of technologies, including data tools, software applications, and hardware, within undergraduate statistics education. Literature on educational technology from the fields of mathematics and physics is also examined to gain broader perspective on general principles for development and application of educational technology.  

\subsection{Role of Technology in the Statistics Education Reform Movement}

The role of technology in the undergraduate statistics classroom expanded in the wake of the statistics education reform movement of the mid 1990s. The reform movement was a push to modernize curricula in terms of  what statistics courses taught and how they approached the material. \citet{Moore1997} chronicled the pedagogical shift toward constructivism and democratization within the statistics classroom in the 1990s. Introductory statistics was dramatically altered during this movement with data collection, graphical display and a focus on application and interpretation added to the previously theory oriented curriculum. Moore argued that this shift in pedagogy and content needed to be accompanied with a larger focus on using technology in order to maximize student learning. 

The statistics education reform movement of the 1990s changed the landscape of statistics education. Teaching practices advocated for by reformers are summarized by a panel of experts in statistics education selected by the American Statistical Association in the \textit{Guidelines for Assessment and Instruction in Statistics Education (GAISE): College Report} \citep{GAISEcollege}. It should be mentioned that another panel of experts also convened under the GAISE name to compile a list of recommendations for developing a Pre-K-12 curriculum framework for statistics education and hold similar recommendations as those found in the College Report \citep{GAISEk12}.  The GAISE College Report provides the following six recommendations for teaching introductory statistics courses:

\begin{enumerate}
\item Emphasize statistical literacy and develop statistical thinking
\item Use real data
\item Stress conceptual understanding, rather than mere knowledge of procedures
\item Foster active learning in the classroom 
\item Use technology for developing conceptual understanding and analyzing data
\item Use assessment to improve and evaluate student learning
\end{enumerate}

The role of technology in these principles is explicit in item 5 and it can be argued that technology plays a supporting role in all other recommendations. Since the GAISE guidelines were proposed, technology has been increasingly implemented by statistics educators. Hassad surveyed 227 introductory statistics faculty members on their attitudes and use of technology in the classroom in 2005 then again in 2013. He found that technology use in statistics curricula has been increasingly embraced by the majority of statistics faculty. For instance, 76\% of faculty report ``involving students in using a statistical software program'' in 2013 as compared to 50\% in 2005 \citep{Hassad2013}. These works illuminate why the role of technology in the statistic classroom underwent massive changes since the early 1990s, but the question of how those changes unfolded is a complex story.

To evaluate how technology has evolved in recent years, we begin with existing reviews on the subject. \citet{chance2007} reflect on how the statistics education reform movement impacted the role of technology in the classroom. They evaluated that, as of 2007, technology was being widely used to help with educational administration through course management software. Currently, the most popular learning platforms include Blackboard \citep{Blackboard}, Moodle \citep{Moodle} and Desire2Learn \citep{Desire2Learn} that account for 44\%, 23\% and 11\% market share, respectively \citep{CampusComputing}. Chance et al. found that general purpose statistical analysis software, such as \textit{R} \citep{R}, \textit{SAS} \citep{SAS}, \textit{SPSS} \citep{SPSS}, \textit{STATA} \citep{STATA} and \textit{Minitab} \citep{Minitab}, had gained a large footing within statistics curricula to support the increased focus on applied data analysis. They also found that software and hardware had developed specifically to facilitate the change in content and pedagogy \citep{chance2007}. \citet{Rubin2007} evaluated the changing trends of technology in statistics education from 1992 to 2007. She illustrated how several software programs were implemented to introduce new data and graphical displays to the statistics classroom. She concluded that although new data sources and user interfaces allow for a more advanced methods for statistics education, problems with student understanding persist, stating ``a general caution for readers; as amazing and inspiring as these technologies may seem, none of them can have any educational effect without carefully constructing curriculum and talented teaching'' \citep[p.2]{Rubin2007}. 

\subsection{Statistics Education Technologies} 

We now inspect specific technologies used in statistics education.  To reflect the current field of statistics education, this review on specific technological tools narrows in scope to literature and tools produced, or at least updated, within the past decade. % Many of the articles comes from the Technological Innovations in Statistics Education (TISE) journal, which is a deep source of literature on this topic.
Literature is organized into topics of data oriented technologies, educational software, statistical and mathematical software extensions, and hardware used for statistics education.

\subsubsection{Data Technologies} 
\label{DataTech} 

The second GAISE principle argues that the use of real data for statistics education is superior to using fabricated data because it fosters student engagement  and the context emphasizes how and why data is collect and analyzed \citep{GAISEcollege}. This call for using authentic data is backed by \citet{Neumann2013} who conducted open-ended student interviews in a course implementing only real-life data sources. Students were asked for their general thoughts on the use of real data and several themes were recurring in the answers from large percentages of the students: real life relevance (63\%), interest (58\%), learning and memory (50\%), motivation (37\%), engagement (32\%), understanding (24\%). \citet{Finzer2007} argue that data needs to be engaging and interesting in addition to being real.  They state ``(w)hat seems to be missing (in introductory statistics courses) are data sets -- especially large and highly multivariate data sets -- that are ripe for exploration and conjecture driven by students' intrigue, puzzlement and desire for discovery'' \citep[p.1]{Finzer2007}.  

While some educators may have access to real data from their own research projects, many need to seek data from other sources to use in their courses. Online data repositories exist to facilitate this search for data. Curators of data repositories gather data on many subjects then provide data descriptions and easy access. Hundreds of data repositories exist to organize data on various subjects. 

There are data repositories curated for statistics education, where data sets are organized by applicable statistical topics. These include Consortium for the Advancement of Undergraduate Statistics Education Online Resources \citep{CAUSErepo}, Many Eyes \citep{ManyEyes}, Journal of Statistics Education Data Archives \citep{JSErepo}, The Data and Story Library \citep{DASL}, and Australasian Data and Story Library (OzDASL) \citep{OzDASL}. Massive data archives from other fields of study have been collected and made available through universities, such as University of Michigan's Inter-university Consortium for Political and Social Research \citep{ICPSR}, University of Connecticut's Roper Center Public Opinion Archives \citep{Roper}, and the General Social Survey by the National Opinion Research Center at the University of Chicago \citep{GSS}. The multitude of data repositories has led to web directories entirely devoted to organizing links to repositories, such as the Statistical Science Web \citep{SSW} and the University of Illinois -- Urbana's Applied Technologies for Learning in the Arts \& Sciences \citep{ATLAS}.  With this plethora of data sources, statistics instructors hardly have an excuse to use boring or fake data examples within classes. 

There are other major sources of data for the classroom that are free to access but require more technical skills to retrieve.  The Freedom of Information Act was expanded in 1996 to ensures that non-classified data from the United States Government is accessible online \citep{FOIA}.  The online government resources at \url{www.data.gov/} \citep{DataGov}, \url{www.census.gov/} \citep{Census}, \url{www.nhtsa.gov/} \citep{NHTSA}, and \url{www.cdc.gov/} \citep{CDC} are all locations of massive data stores. Other large online databases have also been made available by non-governmental organizations such as public health data from the World Health Organization \citep{WHO} and poverty data from the World Bank  \citep{WorldBank}.

Schutes presents a related idea on how to stretch a single data set into individualized data sets for students using a random sub-selection process. Sub-selecting using automation gives all students data with identical context and interpretability, but different subsets of data values. This is ideal for projects where  students are asked to individually complete a task but have the freedom to discuss  context and approach freely with classmates \citep{Schutes2009}.  This topic could be extended to include the automated generation of solution sets from each individualized data set using the \texttt{knitr} package in \textit{R} \citep{Yihui}.

The enormity and accessibility of  data is a wonderful opportunity for statistics education but \citet{Nicholson2013} argue that it does not improve anything in statistics education unless educators actually use it. \citet{Ridgeway2013} propose use of the ``symantic web'' as a framework to support the use of large data but seem to fall short of convincingly arguing how this technology would get better multivariate data into classrooms. \citet{Finzer2007} argue that large real-life data often fails to make it into the classroom because educators tend to lack the technical skills and experience for getting those types of data into the classroom. The combination of these opinions seems to indicate that statistics educators must be technologically savvy to seize the opportunities that large data can provide.

\subsubsection{Educational Software} 

Many software applications have been developed specifically for statistics education. The following review is not meant to serve as an exhaustive list of all software applications, but is a representation of the tools being employed in statistics education in recent years. Some applications are learning objects focused on teaching statistical methodology. One such tool is \textit{The Island}, an online data collection simulator which allows students to collect medical records or conduct experiments with virtual residents of a fiction island \citep{Bulmer2011}. In order to emphasize a realistic portrayal of experimental design and data collection, \textit{The Island} requires students to collect data in a laborious step-by-step manner, instead of providing a simple one-step data pull. \citet{BaglinBedfordBulmer2013} conducted a student survey about using the tool and found that students perceive \textit{The Island} to be engaging, easy to use and beneficial to their learning. Another software tool, \textit{GeoVista} was implemented to bring Geographic Information System (GIS) data into a statistics course \citep{Forbes2012}. Other software packages have been developed to help students understand mathematically complex probability concepts. \textit{Tinkerplots} has been developed as a tool for teaching students about the model fitting process and how to connect data collection to the probability concepts used for analysis \citep{Konold2008}. The \textit{Probability Explorer} software allows for students to simulate discrete outcomes from theoretical distributions with the goal of teaching the interplay of ideas that knowing empirical observations inform on how to model the theoretical distributions and knowing the theoretical distribution informs on what to expect from empirical observations \citep{Lee2009}.

Online discussion boards and multimedia integration have been implemented widely within online and traditional classrooms. Although not specifically developed for statistics education, literature on the use of these tools within statistics education yields quality insight. \citet{Everson2008} implemented a completely online course in introductory statistics with heavy emphasis on discussion boards. They give many administrative pointers on how to ensure that online discussion boards are beneficial to student learning. \citet{Schmidt2013} also discusses how to foster meaningful discussion about statistical concepts based on experience in an introductory biostatistics course.  She finds that ``(t)he main challenge to implementing these activities is the time commitment required to read and respond to posts... but the opportunities and benefits far outweigh this"  \citep[p.9]{Schmidt2013}. 

Multimedia tools integrate audio, video, images and reading materials into learning modules to engage students in a comprehensive manner. \citet{McDanielGreen2012Paper1} have developed \texttt{i\string^3}, an open-source Java applet for teaching distributional, inferential and probabilistic concepts.  In another paper, \citet{McDanielGreen2012Paper2} use pre/post testing to find that using \texttt{i\string^3} to supplement traditional instruction increases student understanding of sampling distributions. \citet{Harraway2012} researched the pedagogy of combined use of multimedia and the \textit{GenStat for Teaching and Learning (GTL)} software. He argues that the implementation of this combined approach with New Zealand high schoolers was successful with the multimedia instruction and menu driven software allowing students to focus on learning statistical concepts instead of syntax details and programming.

\subsubsection{Statistical and Mathematical Software Extensions} 

Research has also been conducted on employing existing statistical and mathematical software for the purposes of statistics education. \textit{R} has been utilized in a number of ways within statistics education; through package development, data analysis and extensions. An example is the \texttt{LearnBayes} package \citep{LearnBayes} developed by \citet{Albert2009} for teaching basic discrete Bayesian modeling. Another option through \textit{R} is for instructors to make web based applications through the \texttt{shiny} package that are tailored to their specific course needs \citep{shiny}. The \textit{Visual Interactive Statistical Analysis (VISA)} program is built on top of \textit{Excel} \citep{Excel} to give students menu driven functionality; however, the authors are not convincing in arguing that the \textit{VISA} interface adds much pedagogical value over the underlying \textit{Excel} programming \citep{Shaltayev2010}. 

\citet{Hoff2012} employ \textit{Mathematica} \citep{Mathematica} to teach students about the  Central Limit Theorem; specifically how the interplay between sample size and population distribution effect the normal approximation of the true sampling distribution for the sample mean. Unfortunately, the implementation within an assignment amounted more to a test of technical computing skill than a learning opportunity. A visual comparison of the normal approximation and the true sampling distribution would provide a more direct way to establish the conceptual understanding.  \citet{Harlow2009} designed the \textit{Visual Cognitive Tool (VCT)}, a graphical user interface built upon \textit{MATLAB} \citep{MATLAB}, to teach mathematical statistics.  The \textit{VCT} program allows students to visualize complex probabilistic concepts, such as Galton's binomial Quincunx and the trinomial Septcunx. These applications can expose students to an understanding of analysis software that holds value in a future professional setting, but it is of foremost importance that they hold pedagogical value for learning statistical concepts.

\subsubsection{Hardware} 

There is little literature on technological hardware developed specifically for statistical education due to the fact the most technological tools used in the classroom are software. There are however a few notable handheld electronic devices being used within statistics classrooms; graphing calculators and clickers. The graphing calculator is falling out of favor at the university level but is still the only allowed tool for statistical computation for Advanced Placement Exam following Advanced Placement statistics courses in American high schools \citep{APcalculator}. Graphing calculators and wireless response ``clickers'' have also been implemented with some success within large lecture sections of introductory statistics courses as an attempt to drive student engagement and learning through involvement in a data simulation process \citep{Kaplan2011}.  A large scale designed experiment at University of Michigan exposed 48 sections of an introductory statistics course to combinations of treatment by \citet{McGowanGunderson2010}. They found little evidence that clickers drive engagement but some evidence that they improve learning. They emphasize that the technology alone will not improve student learning unless it is thoughtfully used for a pedagogical reason.

\subsection{Educational Technologies in Similar STEM Disciplines}
\label{EdTechSTEM}

To gain perspective on the use of technology in statistics education, we may also look to the development and use of educational technology in other STEM fields. This exploration will consider software and hardware that are either designed for the classroom or professional tools applied within the classroom. For most direct comparison, we discuss two disciplines that strongly resemble statistics: mathematics and physics. Following the specific examples educational technologies within each field is a discussion of general principles of development and application of technology that can extend to statistics education. 

\subsubsection{Educational Technologies in Mathematics}
\label{EdTechMath}

The National Council of Mathematics Teachers periodically issue guidelines for effective mathematics education -- akin to the GAISE guidelines in statistics education -- that emphasize the importance of integrating technologies in mathematics curricula \citep{nctm2000standards}. The widespread use of electronic technology in mathematics education started in the 1960's with the accessibility of four-function calculators, then followed by scientific calculators \citep{trouche2010handheld}. Early implementation of calculator use was limited to the role of automating computation. In the late 1980's and early 1990's handheld technology in the mathematics classroom advanced substantially with the advent of the programmable graphing calculator. Curricula started to be developed with the graphing calculator as part of pedagogy. \citet{burrill2002handheld} state that ''Integrating, not simply adding, the use of handheld graphing technology within the context of the mathematics being studied can help students develop essential understandings about the nature, use, and limits of the tool and promote deeper understanding of the mathematical concepts involved." \citet{kaput1994technology} use a wave metaphor to characterize the research on technology in mathematics educational from the 1970's to the 1990's; with \textit{wave level} studies quantifying the improvement in computational precision, \textit{swell level} studies exploring the cognitive advantages of learning with technology, and \textit{tidal} changes in mathematical curricula.  

The modern mathematics classroom has greatly expanded the type and power of digital technologies in the past decade. The graphing calculator still plays an important role but many other digital teaching tools have emerged, most notable being the plethora of software developed for use on general purpose computing hardware \citep{trouche2010handheld}. The many specialized software applications are developed for specific mathematical learning objectives. Examples abound in geometry (e.g. CaRMetal \citep{CaRMetal}, Cinderella \citep{cinderella}, The Geometer's Sketchpad \citep{jackiw2002geometer}, and 3-D Applets \citep{boon2009designer}) and algebra (e.g. Digital Mathematics Environment \citep{DME}, Maple TA \citep{MapleTA}, Webwork \citep{gage2002webwork} and Wims \citep{wims}). Also, many general use mathematics software programs have been increasing incorporated into the mathematics classroom; such as Mathematica \citep{Mathematica}, Maple \citep{Maple}, Wolfram Alpha \citep{wolframalpha}, Sage \citep{sage}, and MATLAB \citep{MATLAB}. Most recently, widespread ownership of smart-phones has also been harnessed for mathematics education. For instance, the MobileMath game was developed for younger students to learn geometric concepts through a smart-phone GPS orienteering game \citep{wijers2010mobilemath}. 

With the large scale development of digital technologies for mathematics educations there has been growing research into properly designing and evaluating these tools. \citet{drijvers2012digital} argues that the design of technology for education must also include the design of accompanying activities. This philosophy is argued to lead to software that more effectively integrated into curricula \citep{freiman2014technology}. \citet{stohl2000promoting} provide five guidelines for technology-based activity development: (1) introduce technology in context, (2) address worthwhile mathematics with appropriate pedagogy, (3) take advantage of technology, (4) connect mathematical concepts, and (5) incorporate multiple representations. Research is also being done to formally evaluate existing digital technologies. For instance, \citet{bokhove2010digital} evaluate several of the algebra learning objects mentioned above using a set of criterion constructed for digital tools in mathematics education; finding the Digital Mathematics Environment \citep{DME} to be the strongest tool based on adherence to quality software and pedagogical design. 

\subsubsection{Educational Technologies in Physics}
\label{EdTechPhysics}

\citet{flick2000preparing} provide general guidelines for using technology in science education -- echoing the five principles from \citet{stohl2000promoting} nearly verbatim -- that advise for technology to be introduced within context and leverage appropriate pedagogy to help develop students' understanding of scientific concepts. \citet{rios2000guide}, and more recently \citet{bryan2006technology}, have provided overviews on the use of technology within physics education. They both highlight the three primary applications of technology: computer interfacing equipment used to collect and process data, modeling, and graphical simulation. Conducting physics experiments allows students to actively explore the mathematical concepts through collected data. This is often done by digitally collecting measurements with physical sensors (e.g. Calculator-Based Laboratory \citep{cbl}, and PASPORT Probeware \citep{pasport}) or through the use of video analysis software (e.g. Logger Pro \citep{loggerpro}, and Tracker \citep{tracker}). Learning objects for modeling or graphical simulation are often developed as web-based applets for accessibility (e.g. Physics Education Technology (PhET), Interactive Simulation \citep{PhET} and Physion \citep{physion}). In his overview, \citet{bryan2006technology} concludes that these technologies can aid in student learning and are most successfully implemented when they are developed to adhere to the pedagogical principles and clearly incorporate curriculum concepts. 

% \subsection{General Principles for the Development and Application of Educational Technologies}
% \label{GenPrinciplesEdTech}
% n
% The literature in Section\ref{EdTech} provides a 

\section{Comparison of Statistical Inference Curricula} 

The research done in Chapter 2 of this dissertation is a designed experiment to compare the learning outcomes of students receiving traditional and simulation-based curricula for teaching statistical inference in an introductory statistics course.  The term \textit{traditional} refers to a curriculum that introduces statistical inference through probability distributions and application of mathematical theory in empirical scenarios. This approach is characterized by the use of distributional approximations, cumulative probability tables and formulas for confidence intervals and test statistics. In contrast, the \textit{simulation-based} approach to teaching statistical inference relies on methodology driven by resampling and permutation instead of theoretical probability distributions. This allows for the concepts of statistical inference to be learned through methods such as bootstrap confidence intervals and randomization testing of hypotheses. The following subsections will outline the existing literature on simulation-based inference curricula and their comparison to traditional inference curricula, then explore comparative studies within statistics education and lastly discuss assessment tools designed for measuring student learning outcomes.


\subsection{Simulation-Based Inference} 

George Cobb is widely regarded as the progenitor of the recent call to use simulation-based methods for teaching introductory statistics. Cobb argued that calculation and graphics were automated and streamlined through an expansion in the use of technology during the statistics education reform movement but nothing was done to change how inference concepts were being taught. Computation has become so cheap and widely available that simulation-based methods that rely heavily on repeated simulation are now becoming practically feasible. He also argues that the traditional theory-based inference methods continue being taught, not because they are better, but because historical momentum drives to their perpetuation. He uses the shift from a earth centered to a sun centered view of the solar system as a metaphor for the the shift from tradition to simulation-based methods for teaching inference; it is not only simpler but it is more accurate to the core concepts of inference \citep{Cobb2007}.

Since 2007 a growing number of educators have joined Cobb in advocating for teaching simulation-based methods in introductory statistics courses. There are a few established groups of authors producing course materials to support the wave of educators adopting the simulation-based approach to inference. \textit{Statistics: Unlocking the Power of Data} is a textbook that follows a traditional progression through data collection, summarization and graphical topics, then uses randomization tests and bootstrap confidence intervals to teach the concepts of statistical inference \citep{Lock5}. The \textit{Introduction to Statistical Investigation} textbook approaches the restructure of the entire introductory course by using the ``spiral approach'' to introduce statistical inference from the very beginning of the course, then adds complexity and related ideas as students repeatedly revisit the inference process \citep{ISI}. Although not in a textbook format, the \textit{Change Agents for Teaching and Learning Statistics (CATALST)} group has a large online collection of freely available lesson plans and course materials that use the simulation-based approach to teaching inference \citep{CATALST}. 

Simulation-based inference has been gaining popularity among statistics educators in recent years.  At the $9^{th}$ International Conference on Teaching Statistics (ICOTS9) more than a dozen educators presented on their research or experience with simulation-based inference curricula \citep{ICOTS9}. There are only a small number of published studies available on the efficacy of the simulation-based approach. \citet{Carver2011} offers a case study on the use of a curriculum structure that heavily integrated simulation-based teaching, but provided no evidence that the methods were superior to the traditional approach. \citep{Budgett2013} also attempt to argue that students' inferential learning is improved by using a simulation-based methods using pre/post test data on student learning outcomes; but again there is no data from students in a traditional curriculum on which comparisons of the curricula can be made. 

Quality comparisons of students in traditional and simulation-based inference curricula have been made in two articles by authors associated with the \textit{Introduction to Statistical Investigation} textbook. Both studies measured learning outcomes, through pre/post testing, from groups of students at Hope College who received either a traditional or simulation-based approach to inference during an introductory statistics course. One study found that students of the simulation-based inference had a larger improvement in understanding of hypothesis testing than the traditionally taught students \citep{Tintle2011}. The other study used a follow-up examination four months after the course ended to test memory retention and found that students of the traditional inference curriculum had significantly worse retention of inference concepts \citep{Tintle2012}. 

\subsection{Experiments in Statistics Education} 

The pair of studies by Tintle et al. present strong preliminary evidence in support of using simulation-based methods for teaching statistical inference. However, these studies suffer from a common problem in educational literature; educational conditions are administered to entire classes of students but comparisons are made -- potentially improperly -- using measurements from individual students (\citealt{Ragasa2008}, \citealt{BaglinDaCosta2013}, \citealt{Williams2012}, \citealt{Carlson2011}). Using experimental design terminology, we would consider the class of students in these studies the experimental units and the individual student the observational unit. The issue is that causal inference cannot be made about the treatment effects on the observational units, only on the experimental units.  In other words, in the Tintle studies there is no guarantee that the curriculum treatment led to the improvement in understanding and retention scores for students, because the treatment effect is inextricable confounded with all other class related factors.

There are two approaches that can remedy the mismatch of the experimental and observational units; randomly assign multiple classes to each treatments then compare whole class measurements, or randomly assign multiple students to each treatments then compare individual student measurements. The former requires a large investment of resources because it involves multiple classes of students. This approach was used in a study on the effectiveness of clicker use in an introductory statistics course at University of Michigan where 48 sections -- each section containing 25 students -- were randomly assigned in a 4X3 full factorial designed experiment and thus had four sections per comparison groups \citep{McGowanGunderson2010}. \citet{McGowan2011} reminds the reader that if this design is used, then random assignment of multiple classes to each comparison group is necessary so that treatment effects are not confounded with group factors. 

The alternative, randomly assigning individual students to treatment groups, has not been utilized in statistics education research. This is in large part due to logistical impracticality of randomly allocating students to different treatments in a comparison of educational techniques. Either multiple educators are needed to handle the comparison groups simultaneously, or students must be reassigned to different class times; both options are far from ideal in a university settings. The experimental design for our curriculum study was able to accomplish random assignment of students to curriculum without affecting scheduled class times through an innovative combination of co-teaching and room scheduling.

There are arguments to be made against the use of experiments in educational studies. \citet{Cook2002} argues that experimental logic strongly supports causal conclusions but outlines several practical reasons why he believes they are not used in education.  He argues that there are often insurmountable problems with properly implementing experiments in schools, results tend to trade external validity for causal isolation, and that often observational studies may establish strong enough associations to make the necessary policy decisions. There are also those who are more philosophically opposed to experiments in education. \citet{Howe2004} argues that the quantitative methods used in experimentation are not capable of measuring student learning in any meaningful way and that qualitative methods are the only appropriate approach to assess learning. Aside from his astoundingly imprecise and inept criticism of the experimental methodologies, his assertion that we cannot quantify understanding or learning is fundamentally incorrect. Educators -- perhaps imperfectly -- regularly quantify student knowledge through the grading of assessments and substantial research has been done on tools designed to measure student learning in statistics.

\subsection{Assessments for Statistical Inference Learning Outcomes} 

In the experiment comparing simulation-based and traditional statistics inference curricula we need a valid metric for student learning outcomes. There exist a number of tools developed to measure students learning of introductory statistics concepts, some specific to statistical inference. The \textit{Comprehensive Assessment of Outcomes in a first Statistics course (CAOS)} test is a general purpose test that evaluates student understanding in a wide range of introductory statistics topics \citep{DelMas2007}. The \textit{Reasoning about P-values and Statistical Significance (RPASS)} scale was developed to assess student understanding of statistical concepts surrounding hypothesis testing \citep{LaneGetaz2013}. The \textit{Assessment Resource Tools for Improving Statistical Thinking (ARTIST)} project which produced the \textit{CAOS} test also produced several banks of topic specific questions for testing understanding of particular concepts in introductory statistics. These ARTIST question sets were developed through an iterative process using the \textit{Context, Input, Process, Product (CIPP)} evaluation model to improve the quality of learning assessment \citep{Ooms2008}. The intent of our study is to compare the learning outcomes specific to hypothesis testing and confidence intervals, so we elected to use the ARTIST questions sets for these two topics as measures on which to make comparisons of learning outcomes from the statistical inference curricula. 

\section{Development of the Shiny Database Sampler}
\label{ShinyDBSamplerLit}

Chapter 3 of this dissertation details the construction and evaluation stages in the development of the Shiny Database Sampler; a point-and-click web-based software tool for selecting random subsamples from large databases. The Shiny Database Sampler was created to help fill the void in data technologies discussed in Subsection~\ref{DataTech}, connecting introductory statistics students to available -- but difficult to access -- large data sources. This section highlights pertinent literature, software, and data sources used in the development of the Shiny Database Sampler.

The goal of the tool is to allow students to treat large databases as populations from which to collect random samples through specified sampling schemes. The software also provides basic tools to numerically and visually assess the sampled data prior to download. \texttt{R} \citep{R} was chosen as the computational engine to power the Shiny Database Sampler. \texttt{R} provides all of the sampling and computational functionality necessary many of the simple data operations, but two additional packages were integral to the construction of the web interface and database querying. The \texttt{shiny} package \citep{shiny} allows an \texttt{R} session running on a web server to dynamically generate the JavaScript for the user interface accessed via web browser. \texttt{shiny} operates on the principle of reactive programming, where certain changes to the user interface (editing fields, clicking buttons, etc.) are programmed to trigger \texttt{R} to reprocess data objects which are then incorporated into the JavaScript sent back to the browser for display. This allows for the strength of \texttt{R} for statistical computation to be applied on the server side of a point-and-click web interface. This strength is leveraged in the primary task of collecting random subsamples from a \texttt{MySQL} database \citep{MySQL}. The accompanying query language to interacting with the databases is not simple; certainly no simple enough to expect proficiency from an introductory statistics student. The \texttt{RMySQL} package \citep{RMySQL} allows for database queries to be executed within an \texttt{R} session. By coupling the functionality of \texttt{shiny} and \texttt{RMySQL} we are able to bypass the skill threshold, by allowing the student to set query specifications for random sampling through a simplified interface which automates the query behind the scenes. 

The Shiny Database Sampler uses two of the large governmental databases mentioned in subsection~\ref{DataTech}: the 2001-2009 Fatality Analysis Recording System accident data from the National Highway Traffic Safety Administration (\url{www.nhtsa.gov/FARS}) and the Public Use Micro Sample data from the 2010 United States Census (\url{www.census.gov/}). The accidents database contains over one-millions records from fatal vehicle accidents, each included 29 variables pertaining to the accident. The census database has 26 variables from the census records of over three-million U.S. residents. Selecting a subset from a large database can be time intensive if the records are not properly indexed using \textit{keys} \citep{Schwartz}. The records in the accident and census databases were indexed using hierarchical keys in such a way that prioritizes the speed of simple random sampling, while also greatly improving the efficiency of stratified random sampling. 

Designing the user interface incorporated principles from the fields of software engineering, educational psychology and human computer interaction. \textit{Quality characteristics} are used to define and evaluate the design attributes in software engineering. The ISO 9126 are popular set of definitions that include six quality characteristics: functionality, reliability, usability, efficiency, maintainability and portability (\citealt{bevan1997quality}; \citealt{manuel2002quality}). These elements provide design goals and an evaluation structure for general quality software construction. The design also incorporates \textit{cognitive load theory} to improve the strength of the Shiny Database Sampler as a learning tool for sampling concepts. Cognitive load theory argues that learning is optimized by avoiding the expense of mental resources on tasks that are not helping to create new schema or integrate new knowledge into existing schema \citep{muller2008}. \textit{Human centered design} applies cognitive load theory to the development of educational software; simplifying interface operation to reduce extraneous cognitive load and improve intended learning outcomes \citep{oviatt2006}. 

After an educational software tool is constructed it is important to evaluate its performance. The ISO 9126 quality characteristics provide a basic framework for assess the software, however the evaluation process needs to then be tailored to the specific learning object. In the initial stages of development \textit{heuristic evaluation} -- a non-formal comparison of software to quality characteristics, and \textit{pluralistic walkthroughs} -- testing software with statistically proficient users to gain feedback, can be conducted to evaluate the usability of the software \citep{nielsen1994}. 

A student user survey using Likert scale responses was constructed as a more formal evaluation of the Shiny Database Sampler. The survey items are written to measure three important elements from human centered design: ease of use, connection to course concepts and engagement. Item response theory deals with the difficulties of eliciting latent characteristic using an ordinal rating scale. Ideally the items in the survey will be easy to answer in a way that reflects true belief (low \textit{difficulty}), clearly differentiate examinees with different beliefs (high \textit{discrimination}), and hold high agreement with other items measuring the same latent characteristic (high \textit{reliability}) \citep{demars2010item}.

Cronbach's $\alpha$ (often called \textit{coefficient} $\alpha$), is used to provide a measure the reliability of the items by summarizing the correlation structure between items \citep{cronbach:51}. Higher values of Cronbach's $\alpha$ indicate higher agreement between item responses; \citet{GeorgeMallery2003} and \citet{Nunnally1978} provide commonly used scales for interpreting the reliability based on the value of Cronbach's $\alpha$.  Under Gaussian data with compound symmetry in the covariance structure, the distribution of Cronbach's $\alpha$ can be approximated with an F-distribution \citep{KistnerMuller2004}. 

It is also important to consider \textit{acquiescence}, or a bias toward positive responses, that occurs in self-report data because of a human bias toward the social desirability of affirmation \citep{Furnham1986}. Negatively phrasing half of the items for each latent characteristic, then reversing the scoring, can be used in an attempt to counter-balance this bias. However the resulting double negation may create items that contra-intuitive as measures of positive responses \citep{Friborg2006}. The language of each item must be carefully constructed to attempt to navigate the inherent difficulties with eliciting honest and accurate human responses.

Many tools and concepts were leveraged in the development of the Shiny Database Sampler. Quality characteristics from software engineering, cognitive load theory from educational psychology and human centered design from human computer interaction were fundamental in the construction of the software using \texttt{R}, \texttt{shiny} and \texttt{RMySQL}. Item response theory from statistical survey methodology was used in formal evaluation of the Shiny Database Sampler. 

\section{Loss in Binned Scatterplots}

The final body chapter of this dissertation shifts from educational tools incorporating large data sources to research on visualizing large data. The research focuses on quantifying the loss of information that occurs in the aggregation necessary for creating binned scatterplots. Literature on big data visualization, scatterplot adaptations, binning algorithms, and perceptual psychology is relevant to this pursuit. 

\citet{Cleveland1987} provides an overview of research in statistical graphics, identifying three primary areas of research: methods, computing, and perception. Research in graphical methodology pursues the development of new ways to encode information from data, data summaries, or model elements into graphical aesthetics. Graphical computation research often investigates ways to optimize plot rendering and creation of user interfaces to support graphical exploration by data analysts. Research in graphical perception is necessary to evaluate the merit of new methods of visualization, because the information encoded in the graphical aesthetics is only valuable if human cognition is able to decode that information perceptually. 

Methodology related to bivariate data displays and binned data graphics are relevant to work with binned scatterplots. The scatterplot is a widely used tool for display of bivariate quantitative data, that uses points on a Cartesian plane to visually display the coordinates of each pair. \citet{friendly2005early} discuss the development of the scatterplot and state, ``Indeed, among all the forms of statistical graphics, the humble scatterplot may be considered the most versatile, polymorphic, and generally useful invention in the entire history of statistical graphics.'' They find that many area plots and time series plots arrived in the eighteenth century \citep{playfair}, but early forms of the scatterplot did not emerge until the nineteenth century \citep{Herschel1833}. 

The scatterplot is very effective at visualizing the bivariate distribution of data pairs, but it often fails to overcome a fundamental challenge; scalability \citep{Theus2006}. While the points do not have any area mathematically, they are visually displayed with round glyphs that occupy space on the graphic. \textit{Over-plotting} occurs in scatterplots when two or more points share overlapping graphical space, thus obscuring part of the visual information. This problem becomes more pronounced as the number of points increases, substantially lowering the utility of the traditional scatterplot in ever growing modern data sizes. Many adaptations for the scatterplot have been suggested to overcome this problem, most either use changes to the glyphs representing each point (\citealt{tukey},\citealt{few2008solutions}, \citealt{Keim2010GenScatter}, \citealt{Hao2010visual}, \citealt{Janetzko2013Ellipse}) or through plotting binned aggregations of points (\citealt{sunflowerplots}, \citealt{Carr1987}, \citealt{Wickham2013Bin}, \citealt{Liu2013imMens}). 

Adaptations that alter the individual points may help in cases of mild over-plotting, but plots that employ binning have superior scalability for visualizing truly massive data. \citet{Liu2013imMens} state that ``Visualizing every data point can lead to over-plotting and may overwhelm users' perceptual and cognitive capacities. On the other hand, reducing the data through sampling or filtering can elide interesting structures or outliers.'' The binned scatterplot is an aggregation based plot that is the two-dimensional analog to a histogram, which bins bivariate data and displays frequency through the shade of geometric tiles, as opposed to the height of a bar. Binned scatterplots are constructed by defining a two-dimensional tessellated grid over the range of the data, binning points based on the grid boundaries, then shading matching geometric tiles based on the frequency of points in the corresponding bin. Rectangular binned scatterplots are often referred to as \textit{heatmaps}, which can be traced back to the end of the turn of the twentieth century \citep{Wilkinson2000}.

Specifying the binning algorithm is fundamental to the construction of a binned scatterplot. Binning algorithms in statistical graphics date back to \citet{Pearson1895} who used univariate binning in the process of visualizing the binomial approximation to the normal distribution.  \citet{Carr1987} introduced binned scatterplots using a hexagonal binning grid. \citet{scott1992} uses \textit{mean integrated squared error} (MISE) -- a loss measure of difference between points and bin centers -- to evaluate bivariate binning strategies. He concluding that rectangular binning and hexagonal binning hold similar MISE, both superior to triangular binning. \citet{Wickham2013Bin} argues for fixed width rectangular binning for computational speed and ease. 

\citet{scott1992} explains that the histogram convey frequency and relative frequency which are the essence of a density function. Similarly, the binned scatterplot can be thought of a visual estimate of the bivariate density. The binning and rendering of the plot encode the density information into the tiles through shading, therefore research on perception of color is important for understanding how that information is recovered visually. Several authors have tested graphical perception of colors in designed experiments. \citet{cleveland1984graphical} find that shading and color saturation are less accurate graphical aesthetics than position, size and angle. In repeating Cleveland and McGill's experiment using updated technology, \citet{heer2010crowdsourcing} similarly rate color as less precise for representing quantitative information than position, size and angle. \citet{demiralp2014learning} find that people tend to judge color similarity primarily based on hue and secondarily based on shade. \citet{healey1999large} present evidence to suggest that up to seven distinct hues are simultaneously distinguishable for the average person, but there is more difficulty discerning different shades of the same hue. This all points to the difficulty that is faced in extracting frequency information based on the tile shades in a binned scatterplot, therefore special consideration is paid in Chapter 4 to mapping frequency information to shade.


\section{Literature Themes}

The review of literature identifies important themes, knowledge, and wisdom to be incorporated in the research that follows. The literature on educational technologies has a distinct take-away for work in statistics education. The recursive message resonates clearly; technology presents the opportunity to create a rich learning environment and engage students on a deeper level, however the development and implementation of technology in education requires proper pedagogy and integration into curricula to effectively improve learning. This theme echoes throughout education literature from statistics, and also in the work of our peers in similar STEM disciplines. Great care must be taken when developing or implementing new educational technologies to ensure that students attentions are guided to learning new concepts, not distracted by the technology. These principles lend well to the pursuit of work on simulation-based learning and the construction of the Shiny Database Sampler. In statistical graphics, the literature commonly reminds us that we not only need to develop efficient ways to visualize information, but we also must critically assess the perceptual ability to accurately recover that information. 
 \chaptocbreak











\chapter{Conclusions}
\label{ThesisEnd}

The goal at the onset of this dissertation was to overcome challenges and grasp opportunities associated with large data and technological innovation to advance the fields of statistics education and statistical graphics. The preceding chapters have demonstrated a successful pursuit of this goals through the review of existing literature to guide relevant research topics, the work to address difficult challenges in both fields, and the application of advanced statistical methods and analysis to draw actionable insights on each topic. To conclude, we review the primary results from each of the studies.

The study on traditional and simulation-based statistical inference curricula took an ambitious and statistically rigorous approach to comparing learning outcomes through a designed experiment. The random assignment of students to curriculum allows for the curriculum effect to be separated from the extraneous instructor and classroom effects. The student learning outcomes, measured using the ARTIST scaled questions sets for confidence intervals and hypothesis tests, are analyzed using a bivariate MANCOVA model which controls for pre-treatment factors, including scores from a midterm and lab. The assumption of independence inherent in this model is critically assessed through a thorough simulation study that eases the concern that a violation of the assumption could inflate the Type I error rate. The model found no significant curriculum effect for learning outcomes related to hypothesis tests; despite the added complexity in the simulation-based curriculum. The model does however indicate a statistically significant, 7\% improvement of the simulation-based curriculum over the traditional curriculum on learning outcomes related to confidence intervals, while accounting for pre-treatment covariates. If this result is representative of true learning outcomes related to confidence intervals, then it is not only statistically significant, it is a finding that roars its affirmation of the advocates for simulation-based curriculum reform. 

The Shiny Database Sampler stands as a successful proof of concept that big data sources can be incorporated into an introductory statistics curriculum using technology. The web-based interface is designed to allow students to easily obtain samples from large databases, using random sampling methodology to infuse the software with pedagogical value. Course activities for an introductory statistics course were also developed to incorporate and leverage the Shiny Databases Sampler for learning about sampling concepts. The software was assessed through a student user survey designed to measure the ease of use, connection to sampling concepts and the engagement with the data. Internal consistency of the student responses was used to assess validy of the latent measurements. The survey indicates that students find the Shiny Database Sampler easy to use, notice the connection to sampling concepts and are moderately engaged with the data context.

The research on loss related to the construction of binned scatterplots yeilded interesting exploration of loss properties and several recommendations for binning specification. Traditional scatterplots are fundimentally difficult to scale for large data due to over-plotting which obscures frequency information. Binned scatterplots trade individual points for shaded tiles, sacrificing exact data locations to allow frequency information to be displayed. The spatial loss is measured as the sum of Euclidean distances from points to their bin centers. The frequency information can be rendered precisely through continuous shading, however the perceptual ability to extract that information is imperfect; thus we suggest the use of frequency binning to make the imprecision explicit through the measurement of frequency loss. The exploration of loss properties provides a few main recommendations for binning specifications: use bin dimensions that are scalar multiples of the data resolution, the binning origin should be offset by half the data resolution below the lowest value in each dimension for coarse data, quantile and standard log frequency binning are two viable strategies for dealing with heavily skewed bin frequencies, and four to seven frequency groups should be used if frequency binning is employed.





\documentclass{article}

\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{float}
\usepackage{graphics}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{natbib}
\renewcommand{\ni}{\noindent}
\usepackage{enumerate}
\usepackage{url}
\usepackage{ulem}
\usepackage{amstext}
\usepackage{amssymb}

\setlength{\parindent}{0em}
 

% comment colors causing compilation problems
\usepackage[usenames,dvipsnames]{xcolor}
\newcommand{\hh}[1]{{\color{ForestGreen} #1}}
\newcommand{\km}[1]{{\color{red} #1}}

\newcommand{\V}[1]{\text{Var}\left(#1\right)}

\begin{document}
\tableofcontents
\newpage


%load packages that will be invisible on slides
<<config, echo=FALSE, eval=TRUE, include=FALSE>>=
# Preliminaries
#setwd("C:\\Users\\Karsten\\Documents\\GitHub\\Chapter3ShinyApps")
library(ggplot2) 
library(reshape2)
library(psy)
library(plyr)
library(graphics)
library(gridExtra)
library(xtable)
dat <- read.csv("data/SurveyResults.csv",header=T)
head(dat)
#list of questions that were inversely coded
flipcoded <- c(3,5,6,7,10,11)
# recode to match positive worded response
for (i in flipcoded){
  dat[,i] <- 6-dat[,i]
}
dat$student <- 1:nrow(dat)
#centered version
cdat <- dat[1:12]
for(i in 1:ncol(cdat)){
  cdat[,i] <- cdat[,i]-3
}

names(dat) <- c("Eas1","Con1","Eng1","Eas2","Con2","Eng2",
                "Eas3","Con3","Eng3","Eas4","Con4","Eng4",
                "Section","Student")


#reshape data
meltdat <- melt(dat[,c(1:12,14)], measure.vars=c(1:12))
meltdat$set <- factor(rep(rep(c("Ease","Concept","Engagement"),4),each=nrow(dat)),levels=c("Ease","Concept","Engagement"))
meltdat$set4 <- rep(rep(c("Ease","Concept","Engagement"),4),each=nrow(dat))
meltdat$set4[meltdat$variable=="Con1" | meltdat$variable=="Con3"] <- "Positive Concept"
meltdat$set4[meltdat$variable=="Con2" | meltdat$variable=="Con4"] <- "Negative Concept"
meltdat$set4 <- factor(meltdat$set4, levels=c("Ease","Positive Concept","Negative Concept","Engagement"))
meltdat$centered <- as.factor(meltdat$value-3)
meltdat$value <- as.factor(meltdat$value)
meltdat$question.in.set <- rep(paste("Item", 1:4,sep=" "),each=3*nrow(dat))
meltdatNoNA <- meltdat[which(meltdat$value != "NA"),]
#create short DF of summary stats
sumstats <- ddply(meltdat,.(variable),summarize,
                  mean = round(mean(as.numeric(as.character(centered)), na.rm=TRUE),2),
                    sd = round(sd(as.numeric(as.character(centered)),na.rm=T),2) )

# Set up for fluctuation diagrams
ggfluctuation <- function (table, type = "size", floor = 0, ceiling = max(table$freq, na.rm = TRUE)) 
{
#    .Deprecated()
    if (is.table(table)) 
        table <- as.data.frame(t(table))
    oldnames <- names(table)
    names(table) <- c("x", "y", "result")
    table <- transform(table, x = as.factor(x), y = as.factor(y), 
        freq = result)

    if (type == "size") {
        table <- transform(table, freq = sqrt(pmin(freq, ceiling)/ceiling), 
            border = ifelse(is.na(freq), "grey90", ifelse(freq > 
                ceiling, "grey30", "grey50")))
        table[is.na(table$freq), "freq"] <- 1
        table <- subset(table, freq * ceiling >= floor)
    }

    if (type == "size") {
        nx <- length(levels(table$x))
        ny <- length(levels(table$y))
        p <- ggplot(table, aes_string(x = "x", y = "y", height = "freq", 
            width = "freq", fill = "border")) + geom_tile(colour = "white") + 
            scale_fill_identity() + theme(aspect.ratio = ny/nx)
    }
    
    else {
        p <- ggplot(table, aes_string(x = "x", y = "y", fill = "freq")) + 
        geom_tile(colour = "white") + scale_fill_gradient2(expression(w[ij]), low = "red", mid="white", high = "blue")
    }
    p$xlabel <- oldnames[1]
    p$ylabel <- oldnames[2]
    p
} 
theme_fluct <- theme(panel.background = element_rect(fill = "white", 
                colour = NA), panel.border = element_rect(fill = NA, 
                colour = "grey50"), panel.grid.major = element_line(colour = "grey90", 
                size = 0.2), panel.grid.minor = element_line(colour = "grey98", 
                size = 0.5)) 

var(cdat[,1]+cdat[,4])
4*cov(cdat[,1],cdat[,4])/var(cdat[,1]+cdat[,4])
cov(cdat[,1],cdat[,4])/ (sd(cdat[,1])*sd(cdat[,4]))
2*(1- (var(cdat[,1])+var(cdat[,4]))/var(cdat[,1]+cdat[,4]))
cor(cdat[,1],cdat[,4])
@


%opening
\title{A \texttt{shiny} New Opportunity for Interaction with Big Data in Undergraduate Education}
%\subtitle{ }
\author{Karsten Maurer \\ Iowa State University, Ames, IA, USA}

\maketitle


% \date{Received: date / Accepted: date}

 \begin{abstract}
As the availability of truly massive data sets proliferates it is enticing to incorporate these data sources into the curriculum of an undergraduate statistics course.  Major barriers exist for interacting with big data due to the computationally intense nature of working with large databases.  Difficulties include gaining access to the database, interacting with database management software and obtaining summary statistics or manageable subsamples from the database for student use.  This paper describes a web based application, the Shiny Database Sampler, which allows instructors to bypass these barriers using simple JavaScript based tools constructed using R and the R packages \texttt{shiny} and \texttt{RMySQL}. The Shiny Database Sampler allows instructors and/or students to obtain smaller subsamples from databases, using a variety of random sampling schemes. 
 \end{abstract}

\section{Introduction}

Statistics education has been rapidly evolving in the past decade with respect to undergraduate course curriculum and assessment. Technology has played the role as a catalyst for many of these major changes.  An important change involves how data is accessed and analyzed in the classroom.   The GAISE report \citep{GAISEcollege} laid out six recommendations on how to improve the teaching of introductory statistics; two of which urge statistics instructors to ``Use technology for developing conceptual understanding and analyzing data'' and to ``Use real data''.   There are many software tools and online repositories for instructors to access real data for use in the statistics classroom; such as the Data and Story Library \citep[DASL, ][]{DASL} and its Australian counterpart, OzDASL \citep{OzDASL}, the Data Archives of the Journal of Statistics Education \citep{JSErepo}, CAUSE Web Repository \citep{CAUSErepo}  and Many Eyes \citep{ManyEyes}. These technological tools are wonderful for accessing many real data sets but the majority of the data sets currently available are quite small in scale.\\

\km{ \citet[][p.~1]{Finzer2007} argue that in an introductory level statistics curriculum ``(w)hat seems to us to be missing are data sets-especially large and highly multivariate data sets-that are ripe for exploration and conjecture driven by the students' intrigue, puzzlement and desire for discovery''. Large, real data sources are becoming increasingly available, but tend to be less easily accessible. A major contributor to this trend is the Freedom of Information Act which to ensures that non-classified data from the United States Government is publicly available \citep{FOIA}.  The online government resources at \url{www.data.gov/} \citep{DataGov}, \url{www.census.gov/} \citep{Census}, \url{www.nhtsa.gov/} \citep{NHTSA} and \url{www.cdc.gov/} \citep{CDC} are all locations of massive data stores. Governmental data sets contain rich information related to many socially relevant issues, making them prime candidates for engaging student interest.\\ }

\km{ The goal of connecting undergraduate statistics students with big data sources requires careful consideration to implement.  \citet{jacobs2009} speaks of the difficulties associated with using, "data whose size forces us to look beyond the tried-and-true methods that are prevalent at that time", including scaling computing tasks, avoiding costs of sub-optimal storage schemes and parallel processing.\\ }

\km{New data technologies are needed in order to allow introductory statistics students to interact with big data sources, such as the governmental databases. This paper discusses the construction and functionality of the Shiny Database Sampler; a web-based application that allows students to pull random samples from large databases. Uses of the application within a lab assignment and course project for an introductory statistics course are detailed.  Lastly the responses from a technology user survey of the Shiny Database Sampler from 265 students of an introductory statistics course are analyzed and discussed.\\ }

% \hh{I think the GOLD book would be better than the reference to the Italian journal} In his paper on graphics for large data,  \citet[][p.~129]{Unwin1999} states that ``(t)he definition of large in relation to data is always changing. A data set that required substantial high performance computing one year becomes easily analyzable on a laptop a few years later''. What constitutes ``small data'' or ``big data'' is constantly being redefined in the field of statistics as computation allows us to collect, store and manipulate larger and larger data sets, but what is consistent is the desire to be able to analyze big data. 
% 
% \hh{Include a few data taxonomies here - there is an early one by Huber, \\
% then one by Wegman: Huge Data Sets and the Frontiers of Computational Feasibility
% Edward J. Wegman
% Journal of Computational and Graphical Statistics
% Vol. 4, No. 4 (Dec., 1995), pp. 281-295\\
% I am sure that there are more recent ones. Include them in a table to show how quickly things are changing wrt the size of data.\\
% The overarching goal is that we want to have interesting and relevant data, and that means that it can't be a toy data set.
% }

% \hh{general outline of the intro should be something like
% \begin{itemize}
% \item: Motivation of what do we do?
% \item What has been done? - that's where the part of the lit review comes in
% \item Why is what we are doing relevant, i.e. where are the holes in the lit review - that can be mixed with the previous item
% \item Outline of the structure of the rest of the paper
% \end{itemize}}

%-----------------------------------------------------------------------------------
\section{Shiny Database Sampler}

Exposing students to large data is tricky because after a certain size, issues with transferring, storing and accessing data become too unwieldy to be feasible with a student's personal computer. Remote databases and database querying software puts its use outside the realm of most undergraduate statistics courses for both students and instructors. The Shiny Database Sampler tool was constructed to provide access to data in large databases by allowing users to obtain and download data subsets of a more manageable size.\\

Most introductory statistics courses are focused on small sample methodology, so the Shiny Database Sampler is designed to allow students to take appropriately sized subsets from the databases to practice methodology learned in class. Working with a small static subset from the large database would let the remainder of the database go to waste. Instead the tool allows students to specify a random sampling scheme that will pull the subset from database dynamically. This is done to emphasize the role of random sampling techniques in data collection, an important concept within an introductory statistics curriculum.

%%%-----------------------------------------------------------------------------------
\subsection{Layout and Design}

The Shiny Database Sampler allows the user to randomly sample subsets from remotely stored SQL databases using a point-and-click graphical user interface. The tool is available online through the link at \url{http://shiny.stat.iastate.edu/karstenm/ShinyDatabaseSampler/}. A screenshot of the graphical user interface is shown in Figure~\ref{fig:samplesummarizetab}. \\ %; however these sections contain different options and displays depending on which tab of the application is selected.\\

\km{The Shiny Database Sampler is a JavaScript based online application created using the \texttt{shiny} package \citep{shiny} in the statistical computing language \texttt{R} \citep{R}. The Shiny package uses \texttt{R} code files to generate the graphical user interface that interacts with an \texttt{R} session running on the server.  This was used in combination with the \texttt{RMySQL} package \citep{RMySQL} to allow the \texttt{R} session on the server machine to query the database at the user's request via buttons on the graphical user interface.  \\  }

\km{The interface was designed with a focus on the quality of the software as an educational tool. The field of software development defines six attributes contributing to software quality: Functionality, reliability, usability, efficiency, maintainability, efficiency and portability (\citealt{bevan1997quality}; \citealt{manuel2002quality}). The portability of the Shiny Database Sampler is excellent as it can be accessed online through any JavaScript enabled web browser. The reliability and maintainability of \texttt{shiny} applications is stems from the proper implementation of \texttt{R} and \texttt{shiny} on the web server. The key consideration with respect to efficiency was to optimize the database querying using proper indexing so that sample retrieval occurred almost instantaneously \citep{schwartz2012high}.  \\ }

\km{The graphical user interface contains two tabs, each broken into two panels. The layout is designed for the user to select actions in the side panel and view the results in the main panel. The functionality and usability of the "Sample and Summarize" and "Visualize" tabs are discussed in the following subsections.  \\ }
 
\begin{figure}[h]
\centering
\includegraphics[keepaspectratio=TRUE,width=.95\textwidth]{figure/SampleAndSummarizeWithPanelLabels.png}
\caption{Shiny Database Sampler Layout for "Sample and Summarize" Tab} 
\label{fig:samplesummarizetab}
\end{figure}

\subsubsection{The ``Sample and Summarize'' tab}
%\paragraph{The ``Sample and Summarize'' tab} \hfill\newline

A screenshot of the interface for the \textit{Sample and Summarize} tab, the is shown in Figure~\ref{fig:samplesummarizetab} below.  A sidebar panel which contains all the sampling options and controls and a main panel which contains the data table and brief summary of the sampled data set.\\

The sidebar panel contains several fields and buttons for selecting and executing a sampling plan.  At the top of the sidebar is a drop-down menu to select the database table from which the user wants to take a random subset.  The current version of the tool allows users to access %workout data from an Iowa State fitness club called the RecMilers (\url{www.recservices.iastate.edu/fitness/recmilers}), 
the 2001-2009 Fatality Analysis Recording System accident data from the National Highway Traffic Safety Administration (\url{www.nhtsa.gov/FARS}) and the Public Use Micro Sample data from the 2000 United States Census (\url{www.census.gov/}).\\ 

After selecting the database, the user can choose between taking a simple or stratified random subsample of data from the database. If the user chooses simple random sampling then must specify a sample size; whereas if the user chooses stratified random sampling the strata variable and number of samples per stratum need to be specified. Lastly the user sets the seed for the selection algorithm. Setting the seed may seem to contradict the intention to draw random subsets, however it is included after careful consideration. For classroom settings, it is often desirable that students work with identical data sets for consistency of class discussion and grading, which setting the same seed and the same sampling specifications will accomplish. Obtaining a random sample from the databases is still possible with the additional -- but fairly trivial -- step of first randomly generating a starting seed. \\

Once the sampling setup is specified, the user may click the ``Get My Sample!" button and the randomly selected subsample of the database will be obtained and displayed in the main panel of the interface. Note that the interface only keeps track of the more recently selected subsample, which we will refer to as the \textit{active data}. Lastly, the side panel contains the button to download the selected subsample to a local drive on the user's computer.  The data will be downloaded as comma separated values (csv) file to the default download folder on the user's computer.\\

The main panel of the Shiny Database Sampler interface displays a data table and a basic summary of each variable in the active data. When logging into the webpage a default sample is taken and displayed until a sample of the users choosing is selected. The data table is searchable, sortable and expandable which makes it easy for the user to take a quick peek at the variable names and values that have been selected. The basic summary statistics for each variable are also displayed in the main panel below the data table; those familiar with \textit{R} programming will quickly recognize this as the verbatim output of the \textit{summary} function in \textit{R}. In the case that stratified sampling was used to draw the data, the summary for each variable is broken down by strata. The displays in the main panel of the Shiny Database Sampler are not intended to be the location for any extensive analysis of the active data but instead a quick check that the data that were sampled are what the user intended to select. \\


\subsubsection{The ``Visualize'' tab}
%\paragraph{The ``Visualize'' tab} \hfill\newline

The \textit{Visualize} tab of the interface is designed to construct basic plots of the active data drawn in the \textit{Sample and Summarize} tab. Figure~\ref{fig:visualizetab} shows a screenshot of the layout of the \textit{Visualize} tab. The sidebar panel contains fields for specifying variables and variable types that will be plotted. Univariate plots can be created by selecting the response variable from a drop-down menu containing a list of all variables and the variable type. Plots may also display bivariate relationships by additionally selecting an explanatory variable and it's variable type. The variable can be specified as numerical or categorical, causing the default plotting to generate a histogram or barchart, respectively. Variable types are not automatically specified for variables in the database for the deliberate purpose of forcing student users to consider appropriate ways to display the data.\\

Once the plotting options are selected in the sidebar panel, the user may click the ``Make my Plot!" button to generate and display the plot in the main panel. Table~\ref{tab:defaultplottypes} shows the possible plot types that are created based on the options selected. Since the visualization are intended only for preliminary exploration, the plots have a default construction and labeling that are not able to be customized. \\ 

\begin{figure}[htbp]
\begin{center}
\includegraphics[keepaspectratio=TRUE,width=.99\textwidth]{figure/Visualize.png}
\end{center}
\caption{Shiny Database Sampler Layout for "Visualize" Tab} 
\label{fig:visualizetab}
\end{figure}

\begin{table}[H]
\centering
\begin{tabular}{lll}
Response Variable & Explanatory Variable & Default Plot \\
\hline
Numerical & None & Histogram \\
Categorical & None & Barchart \\
Numerical & Numerical & Scatterplot \\
Categorical & Categorical & Stacked Barchart \\
Numerical & Categorical & Side-by-side Boxplots \\
\end{tabular}
\caption{Plot types supported in \textit{Visualize} tab.} 
\label{tab:defaultplottypes}
\end{table}

%%%-----------------------------------------------------------------------------------
\subsection{Applications}
\label{Applications}

The Shiny Database Sampler was developed for applications in statistics education and was implemented in the initial stages of development within the curriculum of an introductory statistics course. The alpha version of the software was used within one section of Statistics 104: \textit{Introduction to Statistics} for a group lab assignment and a course project. Later, a beta version was extended to be used in the lab assignment for more sections of the same course. These two applications demonstrate possible uses of the Shiny Database Sampler. 

%-------------------------------------------------------------------------
 \subsubsection{Lab Application Overview} 
 \label{LabOverview}

The lab that utilized the Shiny Database Sampler was designed for students to think critically about sampling approaches, then use the tool allowed to treat the large database as 
a population from which to obtain survey data. Students were asked to consider the following pair of hypothetical situations:

\begin{enumerate}
\item Suppose that our goal is to estimate the mean age of all US residents. Similar to polling organizations we have a budget that allows us to survey around 1000 people. To collect our sample we decide to take a simple random sample of 1040 US residents.

\item Suppose now that our goal has changed.  Now we wish to investigate the association between age and state of residency. We want to compare the median ages for different states. We still have a budget that allows us to survey around 1040 people. To collect our sample we decide to take a stratified random sample of 20 residents from each state in the United States plus the District of Columbia and Puerto Rico. 
\end{enumerate}

\hh{XXX what you describe below is called `good friction' when designing computer interfaces, because you need to intentionally slow users down sometimes, and get them to make important decisions first. I am fairly certain that there is a corresponding counter part in the educational literature. Look into literature that talks about how to get students engaged into material - that should be a similar concept to good friction.
}
In each scenario students were asked to discuss the choice of sampling scheme, and in particular to identify potential problems. The students used the Shiny Database Sampler tool to obtain a sample from the database containing a 1\% microsample of the 2010 U.S. Census, from which they estimated mean and median age of U.S. residents. This lab was written to ensure that sampling concepts were the primary focus, with the Shiny Database Sampler acting in a supporting role. In order to avoid (sporadic) clicking of buttons to obtain samples without ever stopping to consider why the sampling approach matters, we intentionally designed the assignment to invite students to carefully consider sampling options {\it before} using the tool. The entire lab assignment can be found in Appendix \ref{labappend}. 


\subsubsection{Capstone Project Application Overview} 
\label{ProjectOverview} 

A second application of the Shiny Database Sampler was as a data source for a capstone project for three sections of the Stat 104 class. Students were required to do work in small groups to complete a capstone project that took a statistical approach to answering a question of their choosing. Groups were allowed to pick their own data source in order to accommodate students' interests in the subject matter of project. The only specific requirement for the project was a written report that explained the collection of bivariate data and an analysis of the association -- included the appropriate plots, inference and interpretations to answer their question of interest.\\ 

Six groups chose to run a mock survey, using the databases available through the Shiny Database Sampler as a stand-in for a large population. These groups were required to demonstrate a strong argument for the sampling plan they used within the Shiny Database Sample. For instance, one group wanted to know if the proportion of fatal accidents involving a drunk driver was higher in California than in Iowa; as part of the report they needed to argue why taking a stratified random sample of 100 fatal accidents per state would produce better information to answer their question than a simple random sample. Groups that gathered random samples from the Census and Accidents databases knew that they had gathered information from real life sources and they seemed genuinely engaged in the results of their projects. \\
 
%%%-----------------------------------------------------------------------------------
\subsection{User Survey for Software Assessment}

\km{As discussed above, the Shiny Database Sampler was designed for student use on course assignments. The quality of the software was initially assessed using a pluralistic walk through \citep{nielsen1994}, where the developer met iteratively with both statistical novices and experts to test the functionality of the early versions of the software. The software improvements that followed this inspection created the beta version of the Shiny Database Sampler that was ready for student use.\\ }

\km{The second stage of assessment for the Shiny Database Sampler was a user survey to learn student opinions about their experience from using the software during the lab assignment described above. Specifically, we are interested in three aspects of the sampler: we want to know if students find the tool easy to operate, if they see the connection to sampling concepts and if they find the data engaging. Exploring the student responses on these three topics of interest helps us to assess the quality of the Shiny Database Sampler as an educational software. \\ }

\km{The ease of use of the Shiny Database Sampler is an considered an important attribute from both an educational psychology and a software development viewpoint. Cognitive load theory postulates that human capacity to process information is limited and that learning is composed of \textit{extraneous load}, effort to overcome obstructions to new knowledge, and \textit{germane load}, effort used to form new schema and integrate ideas with existing knowledge. \citet{muller2008} explain that ``Finding ways to increase germane load and minimize extraneous load has been a central pursuit of researchers under this paradigm." Application of cognitive load theory is extended to the setting of software in \textit{human centered design} \citep{oviatt2006}. The goal is to make interfaces for educational technologies intuitive and easy to operate in order to minimize the extraneous load; thus allowing more mental resources to be devoted to developing and integrating new knowledge. The intuitive construction of the user interface is important component-based software development, where the usability of a tool is defined by its learnability, understandability and operability \citep{manuel2002quality}.\\ }

\km{Active learning requires a high cognitive load and if prior knowledge is lacking, more scaffolding is necessary to support learning \citep{muller2008}. Since sampling concepts are typically new for students, it is important that students clearly recognize sampling concepts in the interface so that it can be used as part of an assignment that is scaffolded for active learning. If students are able to identify the role of the Shiny Database Sampler as a tool for learning about random sampling, it is more easily integrated into the learning process. \\}

\km{Finally, assessing student engagement with the data sources in the Shiny Database Sampler is important because higher student engagement is linked to higher academic performance and learning \citep{carini2006student}. The GAISE guidelines recommend that technological tools should be used to help teach statistical concepts and that the use of real data is important for student engagement, hence we focus on these topics \citep{GAISEcollege}. \citet{Neumann2013} found that students consider real data more interesting and engaging. The hope is that students using the Shiny Database Sampler will find the real nationally collected data sources engaging. \\ }

Student responses to the survey were collected in an anonymous survey following the group lab assignment -- as described in Section~\ref{LabOverview} -- that required students of Stat 104, Introduction to Statistics, at Iowa State University to use the Shiny Database Sampler tool. Six sections of Stat 104 students were surveyed. The students were informed that the survey was not required and that no penalties or rewards were affiliated with its completion.  Of the 320 students attending lab, 265 completed the survey. 

%-------------------------------------------------------------------------
 \subsubsection{Survey Description} 

After completing the lab assignment, students were asked to fill out a survey consisting of twelve statements (referred to as {\it items} in the following, see Table \ref{tab:surveyquestions} for an overview). For each statement, students were asked for feedback on their level of agreement on a Likert scale from strongly disagree to strongly agree. 
The twelve items were designed to assess student opinion within three topics of four items each: ease of use, connection to sampling concepts, and engagement with the census data.  We will refer to these as the Ease, Concept and Engagement item sets.  For each group of four items, two were worded positively and two were worded negatively. Introducing negation with half of the items was done to reduce the response bias associated with \textit{acquiescence}, the tendency to respond positively irrespective of the item content due \citep{Furnham1986}. Responses were scored as -2 (strongly disagree), -1 (disagree), 0 (Neutral), 1 (agree), 2 (strongly agree).  Responses for negatively worded items were reverse-scored for the purposes of analysis,
%Thus the response values are coded to be interpretable as level of agreement with the overall theme of each set; -2(strongly disagree), -1(disagree), 0(Neutral), 1(agree), 2(strongly agree). 


\begin{table}[ht]
\centering
% \scalebox{0.9}{
\begin{tabular}{ccp{3.4in}crr}
Topic Set & ID & Item & Polarity & Mean & SD\\[3pt]
  \hline
Ease & 1 & \it I found the web tool easy to use & $+$ & \Sexpr{sprintf("%.2f", sumstats[1,2])} & \Sexpr{sprintf("%.2f", sumstats[1,3])}\\  
 & 2 & \it The layout of the web tool was intuitive & $+$  & \Sexpr{sprintf("%.2f", sumstats[4,2])} & \Sexpr{sprintf("%.2f", sumstats[4,3])}\\  
 & 3 & \it Using the web tool was difficult & $-$  & \Sexpr{sprintf("%.2f", sumstats[7,2])}& \Sexpr{sprintf("%.2f", sumstats[7,3])}\\  
 & 4 & \it Learning to use the web tool was hard & $-$  & \Sexpr{sprintf("%.2f", sumstats[10,2])} & \Sexpr{sprintf("%.2f", sumstats[10,3])}\\  [3pt]
  \hline
Concept & 1 & \it The web tool helped me understand sampling concepts & $+$  & \Sexpr{sprintf("%.2f", sumstats[2,2])}& \Sexpr{sprintf("%.2f", sumstats[2,3])}\\   
& 2 & \it I understand sampling ideas less after using the web tool & $-$  & \Sexpr{sprintf("%.2f", sumstats[5,2])}& \Sexpr{sprintf("%.2f", sumstats[5,3])}\\  
& 3 & \it Sampling techniques are clearer after using the web tool & $+$  & \Sexpr{sprintf("%.2f", sumstats[8,2])}& \Sexpr{sprintf("%.2f", sumstats[8,3])}\\  
& 4 & \it The web tool made me less sure how to randomly sample & $-$  & \Sexpr{sprintf("%.2f", sumstats[11,2])}& \Sexpr{sprintf("%.2f", sumstats[11,3])}\\  
   \hline
Engagement & 1 & \it I did not enjoy working with the Census data & $-$  & \Sexpr{sprintf("%.2f", sumstats[3,2])} & \Sexpr{sprintf("%.2f", sumstats[3,3])}\\  
& 2 & \it I thought the Census data was boring & $-$   & \Sexpr{sprintf("%.2f", sumstats[6,2])}& \Sexpr{sprintf("%.2f", sumstats[6,3])}\\  
& 3 & \it Knowing that the Census data was from real people made it more interesting & $+$  & \Sexpr{sprintf("%.2f", sumstats[9,2])}& \Sexpr{sprintf("%.2f", sumstats[9,3])}\\   
& 4 & \it I liked analyzing the Census data & $+$  & \Sexpr{sprintf("%.2f", sumstats[12,2])} & \Sexpr{sprintf("%.2f", sumstats[12,3])}\\   
\end{tabular}
% }
\caption{Survey questions and response summaries for all items \textit{after} Reverse-Scoring (RS)}
\label{tab:surveyquestions}
\end{table}

\normalsize



%\newpage
From Table \ref{tab:surveyquestions} we see that all response averages are positive after reverse-scoring. With the Ease items this indicates that students tend to find the tool relatively easy to operate. For frame of reference, we assume that students are comparing the difficulty of use with other educational technologies and webpages they have encountered in the past; in particular the \texttt{JMP} software used previously on their Stat 104 labs and homework. Students also tend to respond to Concept items in a manner that is affirmative that the tool connects them to sampling concepts. Students' responses are near to neutral for most items about engagement with the census data, with the exception of Engagement item 3.  The phrasing of this question seems to have led students to reconsider their engagement level and led to a consistently more positive attitude.

--


%-------------------------------------------------------------------------
\subsubsection{Assessment of Internal Consistency for Item Topic Sets} 

The goal for this survey is to use the responses to sets of items to infer student opinions about the underlying topic of each set.  It is reasonable to aggregate the responses over entire questions sets if we can show that items within each set are measuring the same latent topic. We use fluctuation diagrams and Cronbach's $\alpha$ \citep{cronbach:51} to assess this internal consistency. \\

A fluctuation diagram visually displays a contingency table of a pair of variables as  the area of blocks on the bivariate grid of all possible response values. A diagonal heavy fluctuation diagram indicates strong agreement or {\it internal consistency} between responses of the two items.  Figure \ref{fig:fluctuationDiagrams} contains fluctuation diagrams for all item pairs within topic sets.    
We notice that most pairs of responses fall heavily along the diagonal and are primarily in the upper right of each diagram. This indicates that most items within sets have strong agreement and that the response values are generally neutral to positive for all items after reverse-scoring.
For the item pairs in the Concept topic set we see that fluctuation diagrams have slightly larger off diagonal trends than items within the other two sets, which indicates a lower level of internal consistency for Concept items than in the other two topic sets. 

<<fluctuationDiagrams,echo=F, message=FALSE, include=T,eval=T,fig.width=12, fig.height=6, out.width='.98\\linewidth', fig.pos='hbt',fig.align='center',tidy=F, cache=TRUE, fig.show='hold', fig.cap='Fluctuation Diagrams of All Item Pairs within Topic Sets'>>=
 
#ease of use
e12 <- ggfluctuation(table(cdat$q1,cdat$q4)) +
  xlab("Ease Item 1") + ylab("Ease Item 2") + theme_fluct
e13 <- ggfluctuation(table(cdat$q1,cdat$q7)) + xlab("Ease Item 1") + 
  ylab("Ease Item 3") + theme_fluct
e14 <- ggfluctuation(table(cdat$q1,cdat$q10)) + xlab("Ease Item 1") + 
  ylab("Ease Item 4") + theme_fluct
e23 <- ggfluctuation(table(cdat$q4,cdat$q7)) + xlab("Ease Item 2") + 
  ylab("Ease Item 3") + theme_fluct
e24 <- ggfluctuation(table(cdat$q4,cdat$q10)) + xlab("Ease Item 2") + 
  ylab("Ease Item 4") + theme_fluct
e34 <- ggfluctuation(table(cdat$q7,cdat$q10)) + xlab("Ease Item 3") + 
  ylab("Ease Item 4") + theme_fluct

#concept
c12 <- ggfluctuation(table(cdat$q2,cdat$q5)) +
  xlab("Concept Item 1") + ylab("Concept Item 2") + theme_fluct
c13 <- ggfluctuation(table(cdat$q2,cdat$q8)) +
  xlab("Concept Item 1") + ylab("Concept Item 3") + theme_fluct
c14 <- ggfluctuation(table(cdat$q2,cdat$q11)) +
  xlab("Concept Item 1") + ylab("Concept Item 4") + theme_fluct
c23 <- ggfluctuation(table(cdat$q5,cdat$q8)) +
  xlab("Concept Item 2") + ylab("Concept Item 3") + theme_fluct
c24 <- ggfluctuation(table(cdat$q5,cdat$q11)) +
  xlab("Concept Item 2") + ylab("Concept Item 4") + theme_fluct
c34 <- ggfluctuation(table(cdat$q8,cdat$q11)) +
  xlab("Concept Item 3") + ylab("Concept Item 4") + theme_fluct

#Engagement with cdata
d12 <- ggfluctuation(table(cdat$q3,cdat$q6)) +
  xlab("Engagement Item 1") + ylab("Engagement Item 2") + theme_fluct
d13 <- ggfluctuation(table(cdat$q3,cdat$q9)) +
  xlab("Engagement Item 1") + ylab("Engagement Item 3") + theme_fluct
d14 <- ggfluctuation(table(cdat$q3,cdat$q12)) +
  xlab("Engagement Item 1") + ylab("Engagement Item 4") + theme_fluct
d23 <- ggfluctuation(table(cdat$q6,cdat$q9)) +
  xlab("Engagement Item 2") + ylab("Engagement Item 3") + theme_fluct
d24 <- ggfluctuation(table(cdat$q6,cdat$q12)) +
  xlab("Engagement Item 2") + ylab("Engagement Item 4") + theme_fluct
d34 <- ggfluctuation(table(cdat$q9,cdat$q12)) +
  xlab("Engagement Item 3") + ylab("Engagement Item 4") + theme_fluct

grid.arrange(e12,e13,e14,e23,e24,e34,
             c12,c13,c14,c23,c24,c34,
             d12,d13,d14,d23,d24,d34,
             nrow=3) 
@



Cronbach's $\alpha$ measures internal consistency between a set of responses by comparing the sum of individual variances to the variance of the sum of the responses. It is defined as follows

\begin{equation} \label{eq:alpha}
\alpha \cdot (K-1)/K =  1- \left . \sum_{i=1}^K \V{Y_i} \right /  \V{\sum_{j=1}^K Y_j},
\end{equation}

where $Y_i$ denotes the response on the $i^{\text{th}}$ survey item ($ i = 1,... , K$), and $K$ is the number of survey items considered for internal consistency. Generally, $K=4$ for the item sets of this survey.
Cronbach's $\alpha$ reaches a maximal value of 1, if there is perfect agreement between items (i.e. all responses to the same item set are identical). In the case that items sets are independent, the internal consistency is measured as $\alpha = 0 $. Cronbach's $\alpha$ is negative in the situation of consistent disagreement between responses and will approach negative infinity if there is perfect disagreement between items.  See appendix~\ref{appendCronbach} for details on the bounds for $\alpha$. 
 \citet[][p.~265]{Nunnally1978} propose that an $\alpha$ of 0.7 or above should be considered as an indication of ``modest reliability''.   \citet{GeorgeMallery2003} provide the commonly used extended scale, displayed in Table~\ref{GMAlphaScale}, for interpreting internal consistency based on Cronbach's $\alpha$. \\

\begin{table}[H]
\centering
\begin{tabular}{ll}
\hline 
Internal Consistency & Range \\
\hline
Excellent &  $[ 0.9 , 1.0 ]$ \\
Good & $[ 0.8 , 0.9 )$ \\
Acceptable & $[ 0.7 , 0.8 ) $\\
Questionable & $[ 0.6 , 0.7 )$ \\
Poor & $[ 0.5 , 0.6 )$ \\
Unacceptable & $( -\infty, 0.5 )$ \\
\hline
\end{tabular}
\caption{Extended Scale for Cronbach's $\alpha$ \citep{GeorgeMallery2003}.} 
\label{GMAlphaScale}
\end{table}

Under the assumption of normally distributed variable, Cronbach's $\alpha$ follows approximately an $F_{\nu_1,\nu_2}$, where $\nu_1 = n-1$ and $\nu_2$ is based on a function of the eigenvalues from the quadratic linear combination of the roots of the variance matrix \citep{KistnerMuller2004}.  %Thus distributionally based confidence intervals are available for Cronbach's $\alpha$, but we are not entirely willing to assume Normally distributed responses to our survey and thus have elected to bootstrap the intervals instead.\\  
Normality is a questionable assumption for the responses in the survey, so we have elected to bootstrap the intervals instead.


Table~\ref{cronbachstuff} displays the point estimates and 95\% central bootstrap intervals for Cronbach's $\alpha$ for each item set from the student survey.  The intervals were created using quantiles of Cronbach's $\alpha$ values from each item set based on 10,000 bootstrap resamples. 
 The results indicate modest levels of internal consistency for Ease and Engagement item sets, and a lower level for the Concept item set. This is in agreement with the findings based on the fluctuation diagrams in Figure~\ref{fig:fluctuationDiagrams}. 

<<cronbach,,echo=F, message=FALSE, include=F,eval=F,fig.width=7.5, fig.height=5, out.width='.49\\linewidth', fig.pos='hbt',fig.align='center',tidy=F, cache=TRUE, fig.show='hold', results='asis'>>=
# coefficientalpha package uses "robust alpha" but does not tell where the estimate comes from
#library(coefficientalpha)

bootstrapalphaci <- function(df,nboot){
  dfnoNA <- na.omit(df)
  bootalphas <- rep(NA,nboot)
  for(i in 1:nboot){
    bootdat <- dfnoNA[sample(1:nrow(dfnoNA),nrow(dfnoNA),replace=TRUE), ]
    bootalphas[i] <- cronbach(bootdat)$alpha
  }
  return(quantile(bootalphas,c(.025,.975)))
}

set.seed(102)
#comment out because of odd printing to latex file
#caease <- cronbach(cdat[,c(1,4,7,10)])
#bootstrapalphaci(cdat[,c(1,4,7,10)],10000)
#cacon <-cronbach(cdat[,c(2,5,8,11)])
#bootstrapalphaci(cdat[,c(2,5,8,11)],10000)
#caeng <-cronbach(cdat[,c(3,6,9,12)])
#bootstrapalphaci(cdat[,c(3,6,9,12)],10000)

CronbachResults <- data.frame(Set=c("Ease","Concept","Engagement") ,
                              alpha=c(0.696, 0.528, 0.719),
                              Lower=c(0.613,0.410,0.643), 
                              Upper=c(0.759,0.637,0.776) )
colnames(CronbachResults) <- c("Set" , "alpha", "Lower Bound", "Upper Bound")
print(xtable(CronbachResults,caption = 'Cronbach\'s alpha for each item set with 95\\% central bootstrap confidence interval', digits=3, label = 'cronbachstuff'), 
      include.rownames=FALSE)
@

% latex table generated in R 3.0.2 by xtable 1.7-3 package
% Tue Nov 04 15:05:46 2014
\begin{table}[hbtp]
\centering
\begin{tabular}{lrc}
  \hline
Set & Estimate & 95\% Confidence Interval \\ 
  \hline
Ease & 0.70 & (0.613 , 0.759)\\ 
  Concept & 0.53 & (0.410 , 0.637) \\ 
  Engagement & 0.72 & (0.643 , 0.776) \\ 
   \hline
\end{tabular}
\caption{Cronbach's $\alpha$ Estimates for each item set with 95\% central confidence intervals based on 10,000 bootstrap samples} 
\label{cronbachstuff}
\end{table}

\subsubsection{Assessment of Polarity Issues}  

We next turn our attention to the polarity of the survey items; specifically we consider that positive and reverse-scored negative items may elicit a different responses. The survey contained six unique item pairs based on topic and polarity combinations. Figure~\ref{fig:OverallSetBars} displays compares the distribution of responses from positive and negative item pairs within topics.  We see strong similarity between positive and reverse-scored negative items in response distributions with the Ease and Engagement item sets. The Concept item set however displays a noticeable difference in response distributions from each polarity.  In particular, we see that students are more neutral toward the positively worded questions. This polarity difference in student responses may explain the lower internal consistency measured by Cronbach's $\alpha$. This might be partly due to the problem that the negation of positive constructs may be linguistically counter-intuitive \citep{Friborg2006}. For instance, students may not interpret the statement ``It is not less clear'' as equivalent to the statement ``It is more clear''.  \\

<<OverallSetBars,echo=F, message=FALSE, include=T ,eval=T,fig.width=9, fig.height=2.5, out.width='.8\\linewidth', fig.pos='hbt',fig.align='center',tidy=F, cache=TRUE, fig.show='hold',fig.cap= 'Item set response distributions'>>=
meltdatNoNA$value <- as.numeric(as.character(meltdatNoNA$value))
qu.summary <- ddply(meltdatNoNA, .(set, question.in.set, value), summarize,
n=length(value)
)
qu.summary$polarity <- rep(c("Pos","Pos","Neg","Neg","Pos","Neg","Pos","Neg","Neg","Neg","Pos","Pos"),each=5)
qu.summary$glyphShift <- .05
qu.summary$glyphShift[qu.summary$polarity=="Neg"] <- -.05

qu.means <- ddply(qu.summary, .(set, value, polarity), summarize, means=mean(n))


ggplot() +
  geom_bar(aes(x=as.character(value-3), weight=means,fill=polarity),data=qu.means,width=.5, position="dodge", size=I(2)) + 
  theme_bw() + scale_colour_brewer("", palette="Set1") + 
  scale_shape_manual("", values=c("-","+")) +
#  scale_shape_manual("", values=15:18) +
#  geom_point(aes(x=as.character(value-3), y=n,  colour=question.in.set, shape=polarity), data=qu.summary, size=I(7),position="dodge") +
  facet_grid(facets=.~set) + 
  xlab("Responses (Post Reverse-Scoring All Negative Item)") + ylab("Count") 
@


To assess whether responses from positive and reverse-scored negative items can be reasonably grouped together within topic sets we turn to principal component analysis.  We decompose the item pairs averages for student responses from the six topic and polarity combinations. The component variances and factor loadings from this decomposition are found in Table~\ref{pc6table}. We argue that the data could be reasonably reduced to four principal components because each of these components explains over 10\% of the variance and together they explain 87.4\% of the total variation.  The uniformly aligned factor loadings for Component 1 reflect the general tendency toward student agreement to all items on the survey.  The factor loadings for Components 2 and 3 displayed in Figure~\ref{fig:PCA6plot} show similar projections for positive and negative item scores for Ease and Engagement pairs but a dramatic separation in the positive and negative item scores for the Concept set.  \\  

<<PCA6,echo=F, message=FALSE, include=T,eval=T,fig.width=5, fig.height=5, out.width='.4\\linewidth', fig.pos='hbt',fig.align='center',tidy=F, cache=TRUE, fig.show='hold'>>=
#average score per student per set
cdat$student <- 1:nrow(cdat)

# Try PCA with 4 values Ease, Engagement, Concept+ and Concept-
Set6Scores <- ddply(cdat,.(student),summarize,
                  EasePos = mean(c(q1,q4), na.rm=TRUE),
                  EaseNeg = mean(c(q7,q10), na.rm=TRUE),
                  ConceptsPos = mean(c(q2,q8), na.rm=TRUE),
                  ConceptsNeg = mean(c(q5,q11), na.rm=TRUE),
                  EngagedPos = mean(c(q9,q12), na.rm=TRUE),
                  EngagedNeg = mean(c(q3,q6), na.rm=TRUE)  )
pc6 <- princomp(Set6Scores[,2:7],na.rm=T)

pc6loadings <- loadings(pc6)[1:6,]
pc6vars <- data.frame( PrinComp = 1:6,
            PropOfVar = (pc6[[1]]^2)/sum(pc6[[1]]^2),
            CumPropOfVar = cumsum((pc6[[1]]^2)/sum(pc6[[1]]^2)) ,
            row.names=NULL)
colnames(pc6vars) <- c("PC" , "Prop. of Var", "Cumulative Prop. of Var")
@

\begin{table}[H]
\centering
\begin{tabular}{lrrrrrrr}
\hline 
& Principal Component & 1 & 2 & 3 & 4 & 5 & 6 \\ 
\hline
Variances \hspace{.1in} & Prop. of Var & \Sexpr{sprintf("%.3f", pc6vars[1,2])} & \Sexpr{sprintf("%.3f", pc6vars[2,2])}  & \Sexpr{sprintf("%.3f", pc6vars[3,2])} & \Sexpr{sprintf("%.3f", pc6vars[4,2])} & \Sexpr{sprintf("%.3f", pc6vars[5,2])} & \Sexpr{sprintf("%.3f", pc6vars[6,2])}  \\
& Cumu. Prop. of Var & \Sexpr{sprintf("%.3f", pc6vars[1,3])} & \Sexpr{sprintf("%.3f", pc6vars[2,3])}  & \Sexpr{sprintf("%.3f", pc6vars[3,3])} & \Sexpr{sprintf("%.3f", pc6vars[4,3])} & \Sexpr{sprintf("%.3f", pc6vars[5,3])} & \Sexpr{sprintf("%.3f", pc6vars[6,3])}  \\
\hline
Loadings & Pos. Ease & \Sexpr{sprintf("%.3f", pc6loadings[1,1])} & \Sexpr{sprintf("%.3f", pc6loadings[1,2])} & \Sexpr{sprintf("%.3f", pc6loadings[1,3])} & \Sexpr{sprintf("%.3f", pc6loadings[1,4])} & \Sexpr{sprintf("%.3f", pc6loadings[1,5])} & \Sexpr{sprintf("%.3f", pc6loadings[1,6])}\\
& Neg. Ease &  \Sexpr{sprintf("%.3f", pc6loadings[2,1])} & \Sexpr{sprintf("%.3f", pc6loadings[2,2])} & \Sexpr{sprintf("%.3f", pc6loadings[2,3])} & \Sexpr{sprintf("%.3f", pc6loadings[2,4])} & \Sexpr{sprintf("%.3f", pc6loadings[2,5])} & \Sexpr{sprintf("%.3f", pc6loadings[2,6])}\\
& Pos. Concept &  \Sexpr{sprintf("%.3f", pc6loadings[3,1])} & \Sexpr{sprintf("%.3f", pc6loadings[3,2])} & \Sexpr{sprintf("%.3f", pc6loadings[3,3])} & \Sexpr{sprintf("%.3f", pc6loadings[3,4])} & \Sexpr{sprintf("%.3f", pc6loadings[3,5])} & \Sexpr{sprintf("%.3f", pc6loadings[3,6])}\\
& Neg. Concept &  \Sexpr{sprintf("%.3f", pc6loadings[4,1])} & \Sexpr{sprintf("%.3f", pc6loadings[4,2])} & \Sexpr{sprintf("%.3f", pc6loadings[4,3])} & \Sexpr{sprintf("%.3f", pc6loadings[4,4])} & \Sexpr{sprintf("%.3f", pc6loadings[4,5])} & \Sexpr{sprintf("%.3f", pc6loadings[4,6])}\\
& Pos. Engaged &  \Sexpr{sprintf("%.3f", pc6loadings[5,1])} & \Sexpr{sprintf("%.3f", pc6loadings[5,2])} & \Sexpr{sprintf("%.3f", pc6loadings[5,3])} & \Sexpr{sprintf("%.3f", pc6loadings[5,4])} & \Sexpr{sprintf("%.3f", pc6loadings[5,5])} & \Sexpr{sprintf("%.3f", pc6loadings[5,6])}\\
& Neg. Engaged &  \Sexpr{sprintf("%.3f", pc6loadings[6,1])} & \Sexpr{sprintf("%.3f", pc6loadings[6,2])} & \Sexpr{sprintf("%.3f", pc6loadings[6,3])} & \Sexpr{sprintf("%.3f", pc6loadings[6,4])} & \Sexpr{sprintf("%.3f", pc6loadings[6,5])} & \Sexpr{sprintf("%.3f", pc6loadings[6,6])}\\
\hline
\end{tabular}
\caption{Summary Statistics from Principal Component Analysis with Six Topic/Polarity Item Pairs} 
\label{pc6table}
\end{table}

<<PCA6plot,echo=F, message=FALSE, include=T,eval=T,fig.width=5, fig.height=5, out.width='.5\\linewidth', fig.pos='hbt',fig.align='center',tidy=F, cache=TRUE, fig.show='hold', fig.cap='Item Pair Loadings on Components 2 and 3 from the Principal Component Analysis with Six Topic/Polarity Item Pairs'>>=
load23dat <- data.frame(pc6loadings[,2:3], compname = row.names(pc6loadings))
require(grid)
ggplot(data=load23dat)+ 
  geom_hline(x=0, colour="darkgray") + geom_vline(y=0, colour="darkgray")+
  geom_point(aes(x=Comp.2, y=Comp.3),size=3.5) + xlim(c(-.73,.6)) + 
  theme_bw() + xlab("Component 2") + ylab("Component 3") +
  geom_segment(aes(x = 0, y = 0, xend = Comp.2, yend = Comp.3), colour="grey40") + 
  geom_text(aes(x=0.825*Comp.2, y=0.825*Comp.3, label=compname)) +coord_fixed() + 
  theme(plot.margin=unit(c(0,0,0,0), unit="cm"))
@



This principal component analysis, with all topic and polarity combinations, suggests that we can reduce the dimensionality by combining the positive items with the reverse-scored negative items for Ease and Engagement topics.  This leaves only the Concept item set separated based on polarity for final analysis.  The decision to combine the responses for Ease and Engagement items also aligns with the higher internal consistency for these item sets as displayed in Cronbach's $\alpha$ values and fluctuation diagrams in Figure~\ref{fig:fluctuationDiagrams}. Thus, we will carry forward with the final analysis using four resulting item sets: Ease, Positive Concept, Negative Concept and Engagement.  \\

\subsubsection{Assessment of Orthogonality} 

The next major consideration is whether the item sets are truly measuring different latent topics and are not redundant. The ability of the survey to separately measure the topics of Ease, Concepts and Engagement can be assessed through the orthogonality of the responses from different item sets.  To check the orthogonality of the sets we conduct another principal component analysis; this time on the average responses for each student from the four item sets -- Ease, Positive Concept, Negative Concept and Engagement.  Items sets will be considered highly orthogonal if the principle component analysis cannot reduce the dimensionality from the four sets. \\  


<<PCA4,echo=F, message=FALSE, include=F,eval=T,fig.width=7.5, fig.height=5, out.width='.49\\linewidth', fig.pos='hbt',fig.align='center',tidy=F, cache=TRUE, fig.show='hold'>>=
#average score per student per set
cdat$student <- 1:nrow(cdat)

# Try PCA with 4 values Ease, Engagement, Concept+ and Concept-
Set4Scores <- ddply(cdat,.(student),summarize,
                  Ease = mean(c(q1,q4,q7,q10), na.rm=TRUE),
                  ConceptsPos = mean(c(q2,q8), na.rm=TRUE),
                  ConceptsNeg = mean(c(q5,q11), na.rm=TRUE),
                  Engaged = mean(c(q3,q6,q9,q12), na.rm=TRUE)  )
pc4 <- princomp(Set4Scores[,2:5],na.rm=T)

summary(pc4)
loadings(pc4)

pc4loadings <- loadings(pc4)[1:4,]
pc4vars <- data.frame( PrinComp = 1:4,
            PropOfVar = (pc4[[1]]^2)/sum(pc4[[1]]^2),
            CumPropOfVar = cumsum((pc4[[1]]^2)/sum(pc4[[1]]^2)) ,
            row.names=NULL)
colnames(pc4vars) <- c("PC" , "Prop. of Var", "Cumulative Prop. of Var")
@

\begin{table}[ht]
\centering
\begin{tabular}{lrrrrr}
\hline 
& Principal Component & 1 & 2 & 3 & 4 \\ 
\hline
Variances \hspace{.1in} & Prop. of Var & \Sexpr{sprintf("%.3f", pc4vars[1,2])} & \Sexpr{sprintf("%.3f", pc4vars[2,2])}  & \Sexpr{sprintf("%.3f", pc4vars[3,2])} & \Sexpr{sprintf("%.3f", pc4vars[4,2])}  \\
& Cumu. Prop. of Var & \Sexpr{sprintf("%.3f", pc4vars[1,3])} & \Sexpr{sprintf("%.3f", pc4vars[2,3])}  & \Sexpr{sprintf("%.3f", pc4vars[3,3])} & \Sexpr{sprintf("%.3f", pc4vars[4,3])}  \\
\hline
Loadings & Ease & \Sexpr{sprintf("%.3f", pc4loadings[1,1])} & \Sexpr{sprintf("%.3f", pc4loadings[1,2])} & \Sexpr{sprintf("%.3f", pc4loadings[1,3])} & \Sexpr{sprintf("%.3f", pc4loadings[1,4])}\\
& Pos. Concept &  \Sexpr{sprintf("%.3f", pc4loadings[2,1])} & \Sexpr{sprintf("%.3f", pc4loadings[2,2])} & \Sexpr{sprintf("%.3f", pc4loadings[2,3])} & \Sexpr{sprintf("%.3f", pc4loadings[2,4])}\\
& Neg. Concept &  \Sexpr{sprintf("%.3f", pc4loadings[3,1])} & \Sexpr{sprintf("%.3f", pc4loadings[3,2])} & \Sexpr{sprintf("%.3f", pc4loadings[3,3])} & \Sexpr{sprintf("%.3f", pc4loadings[3,4])}\\
& Engagement &  \Sexpr{sprintf("%.3f", pc4loadings[4,1])} & \Sexpr{sprintf("%.3f", pc4loadings[4,2])} & \Sexpr{sprintf("%.3f", pc4loadings[4,3])} & \Sexpr{sprintf("%.3f", pc4loadings[4,4])}\\
\hline
\end{tabular}
\caption{Principal Component Analysis with Final Four Item Sets} 
\label{pctable}
\end{table}

Table~\ref{pctable} displays the proportion of variance explained by each of the four principal components and also the loadings for each item set mean that compose each component. The first principal component has similar loadings from all item sets, which we can interpret as the general tendency toward positively scored responses on all items. The second, third and fourth principal components create separation for mean responses of the Negative Concept item set, the Engagement item set and the Ease item sets, respectively. The variances in Table \ref{pctable} reveal that over 10\% of the variation is explained by the fourth component, thus it is necessary to retain all four principal components. This inability to reduce dimensionality implies that average student responses from the four item sets are largely orthogonal. Based on the separation in the loadings and the orthogonality of the principal components, we conclude that the average response scores from the four item set have interpretability as measurements of unique latent topics. \\  


%-------------------------------------------------------------------------
\subsubsection{Survey Assessment Results} 

\km{  In the analysis of student responses, we found that the internal consistency, assessed with Cronbach's $\alpha$ and fluctuation diagrams, is acceptable for interpreting the combined item responses that measure of Ease of Use and Engagement with the census data. We did not have the same certainty with the measures of Concept and therefore split the Concept items into two sets: the Positive and Negative Concept item sets.  This split is supported by the initial principal component analysis of the six topic and polarity item pair scores.  The follow-up principal component analysis on the combined responses for each of the four resulting item sets indicated that the factors were all fairly orthogonal. This ensures us that the survey was effective at eliciting unique characteristics of the user experience. \\  }

\km{ The barcharts found in Figure \ref{fig:OverallSetBars} show that the distribution for each item set is heavily skewed to the left, with the majority of students having the neutral to positive responses. The small bump at the far left of each distribution indicates that there was a small minority of students that expressed negative views. The response distributions indicate that on average students found the application easy to use, found that the tool connected them to sampling concepts and felt moderately engaged with the census data that was accessed with the application.  \\  }

% 
% %-----------------------------------------------------------------------------------
% \section{Shiny Database Aggregator}
% Brief intro the functionality goals for the tool
% 
% 
% %%%-----------------------------------------------------------------------------------
% \subsection{Layout and Functionality}
% To be determined
% 
% %%%-----------------------------------------------------------------------------------
% \subsection{Applications}
% To be determined
% 


%-----------------------------------------------------------------------------------
\section{Conclusions and Future Work}

\km{The Shiny Database Sampler allows point-and-click access to large databases through the mechanism of random sampling. This approach adds pedagogical value over the use of static samples because it allows for course activities to highlight the concepts and process of collecting data through random sampling. The lab and project application of the Shiny Database Sampler within an introductory course were designed to emphasize thought about sampling concepts and the data, not about the software. Toward this scaffolded approach, the design aimed to make the interface easy to use, clearly display sampling concepts and provide engaging data. The student user survey indicates that these goals were met. \\ }

\km{The Shiny Database Sampler could naturally be updated to access additional databases or provide more statistical analysis or visualization options directly within the interface. Also in future work, a similar interface could be developed where the user specifies an aggregation scheme instead of a sampling scheme. This approach would be more true to exploration techniques of big data sources than using random sampling; this would be a distinct -- and perhaps exciting -- departure from the content of a traditional introductory statistics curricula. In such an interface, grouping variables or binning parameters could be used to direct dynamic data aggregation to then be explored; using display such as histograms or binned scatterplots. As data size and ubiquity grow, students would be well served by attempts to thoughtfully incorporate big data into the undergraduate statistics curricula. \\}

%------------------------------------------------------------------
% References
\bibliographystyle{asa}
\bibliography{references}

\newpage
%-------------------------------------------------------
\appendix
%-------------------------------------------------------
\section{Appendix: Lab Assignment}
\label{labappend}

For this activity you will be using a tool called the Shiny Database Sampler to take a random sample of United States residents from US census data. The census data is the Public Use Microdata Sample (PUMS) which is a 3 million person subset of the entire Census data.  For this activity we treat our samples as though they are selected from the full census records.  \\ 
	
We are going to explore how these random sampling plans relate to the goals of a sample survey. The tool will allow you to define either a simple random sampling plan or a stratified random sampling plan. In the following two scenarios we will explore the advantages and disadvantages of these two sampling plans. Access the tool at \url{http://shiny.stat.iastate.edu/karstenm/ShinyDatabaseSampler}. \\	 

\underline{Scenario 1:} Suppose that our goal is to estimate the mean age of all US residents. Similar to polling organizations we have a budget that allows us to survey around 1000 people. To collect our sample we decide to take a simple random sample of 1040 US residents. \\

\begin{enumerate}[(a)]
\item Is this study and example of an experiment or an observational study?  Explain your answer.
\item	Your colleague Bob claims that we are wasting our budget to get only 1040 people using random sampling. He says that we could get 20000 responses to the survey if we invested that money into a mailing campaign in Minneapolis. Explain why the random selection is important.
\item	Another colleague, Jill, asks why we do not stratify by state when we take the sample so that we get 20 people from each of the 50 states along with Puerto Rico and the District of Columbia. Explain why this idea would not create a representative sample to pursue our goal.
\end{enumerate}

Now that we have decided on our sampling plan, let's go collect our data.  The Shiny Database Sampler needs to be told 4 pieces of information in order to collect census records the way you want. (1) Choose the database called ``Census'', (2) select the ``simple random sample'' option, (3) enter a random seed, any number between 1 and 10000, you can do this by rolling a 10-sided die 4 times and (4) lastly tell it that we want ``1040'' random draws. Once you have drawn your samples the page will display basic summary statistics for the variables in the census.\\

\begin{enumerate}[(a)]
\setcounter{enumi}{3}
\item Report the 5-number summary and sample mean age.
\item	Use the 5-number summary to construct a box plot of age.
\item	Go to the ``Visualize'' tab.  Choose age as your Response Variable to Plot.  What type of variable is this?  By clicking on Make My Plot? a histogram of the sample of ages will be displayed.  Describe the shape of the data distribution of age.
\item	Is the relationship between the sample mean and sample median consistent with your description of shape?  Explain briefly.
\item	If our goal was to not only estimate the mean age of all the U.S. residents but also come up with estimates of the median age of all residents in each of the 50 states, plus the District of Columbia and Puerto Rico what is a drawback of using the simple random sample of 1040?  Hint: Set the Data Table to display 100 records per page and go to the page that has ``states'' 10 and 11 (Delaware and the District of Columbia).
\end{enumerate}

\underline{Scenario 2:} Suppose now that our goal has changed.  Now we wish to investigate the association between age and state of residency. We want to compare the median ages for different states. We still have a budget that allows us to survey around 1040 people. To collect our sample we decide to take a stratified random sample of 20 residents from each state in the United States plus the District of Columbia and Puerto Rico. 

\begin{enumerate}[(a)]
\setcounter{enumi}{8}
\item  Explain in general why collecting a stratified random sample is a better plan than a simple random sample for answering this question.  
\end{enumerate}

Now that we have decided on our new sampling plan, let's go collect our data.  The Shiny Database Sampler will need to be told 5 pieces of information in order to collect census records the way you want this time. (1) Choose the database called ``Census'', (2) select the ``stratified random sample'' option, (3) enter a random seed, any number between 1 and 10000, you can do this by rolling a 10-sided die 4 times, (4) select ``state'' as strata variable and (5) lastly tell it that we want ``20'' random draws from each state, plus the District of Columbia and Puerto Rico.  \\

It will take a minute or two to collect these data. It is sifting through millions of records and randomly selecting them from within state groups after all! Once you have drawn your samples you can take a peek at your data set in the main panel of the webpage. You will be able to answer the following questions using the summaries provided on the webpage.  \\

You will notice that the summaries are all broken down by state, but the states are not given names, they are given a code number.  This is done on the census to save computer storage space (saving a ``19'' is much smaller than ``Iowa'').  A list of all the state codes is available at \url{https://www.census.gov/geo/reference/ansi\_statetables.html} (Click on FIPS Codes for the States and the District of Columbia).\\ 



\begin{enumerate}[(a)]
\setcounter{enumi}{9}
\item Report the mean and 5-number summary for the age of the sample from the state of Iowa (\texttt{state = 19}).
\item	Report the mean and 5-number summary for the age of the sample from the state of Alaska (\texttt{state = 2}).
\item	Compare the distribution of ages in Alaska and Iowa using the values from parts j and k.
\item	Making comparisons as we have done above would become tedious if we wanted to compare ages between all pairs of states in the country.  What would be a good way to visually display this information so aid in making these comparisons? Explain your answer.
\end{enumerate}

%------------------------------------------------------------------
\section{Appendix: Technical Implementation of Shiny Application}
\label{appendConstruction}

\textbf{Construction schematics and description of sampling process and plotting process.}



%------------------------------------------------------------------
\section{Appendix: Database Descriptions}
\label{appendDB}

\textbf{To be detailed when databases updated and origins better known.}

%------------------------------------------------------------------
\section{Appendix: Cronbach's $\alpha$ Properties}
\label{appendCronbach}

Recall the form of Cronbach's $\alpha$ from equation~(\ref{eq:alpha}):\\

\begin{center}
$\alpha = \left(K/(K-1)\right) \left( 1- \sum_{i=1}^K \V{Y_i} \right.\left/  \V{\sum_{j=1}^K Y_j}  \right)$,
\end{center}


\textit{Claim 1:} Perfect agreement in items leads to $\alpha = 1$ \\

\textit{Proof:} Let $ Y = Y_1 = Y_2 = ... = Y_k$, thus having perfect agreement.\\

$\Rightarrow$  $\text{Cov}(Y_i, Y_j) = \V{Y} = \sigma^2_y $ \hspace{.1in} $\forall i\ne j$ \\

$\Rightarrow$  $\V{\sum_{j=1}^K Y_j }$ = $\sum_{i=1}^K \V{Y_i} + \sum_{i\ne j}\text{Cov}(Y_i, Y_j)$ =  $K\sigma^2_y + K(K-1)\sigma^2_y$ \\

$\Rightarrow$  $\alpha = \left(K/(K-1)\right) \left( 1- \sum_{i=1}^K \V{Y_i} \right.\left/  \V{\sum_{j=1}^K Y_j}  \right)$ = \\
\indent \hspace{.2in} $\left(K/(K-1)\right) \left( 1- K\sigma^2_y \right.\left/ K\sigma^2_y + K(K-1)\sigma^2_y \right)$ =\\
\indent \hspace{.2in} $\left(K/(K-1)\right)  (1- 1/K)$ = $\left(K/(K-1)\right) ((K-1)/K)$ = $1$ \\

\vspace{.25in} %------------------------------

\textit{Claim 2:} For independent items $\alpha = 0$ \\

\textit{Proof:} Let $Y_1 = Y_2 = ... = Y_k$ be independent 

$\Rightarrow$ $\sum_{i=1}^K \V{Y_i} = \V{\sum_{j=1}^K Y_j} $ \\

$\Rightarrow$  $\alpha = \left(K/(K-1)\right) \left( 1- \sum_{i=1}^K \V{Y_i} \right.\left/  \V{\sum_{j=1}^K Y_j}  \right)$ = \\
\indent \hspace{.2in} $\alpha = \left(K/(K-1)\right) \left( 1- \V{\sum_{j=1}^K Y_j} \right.\left/  \V{\sum_{j=1}^K Y_j}  \right)$ = \\
\indent \hspace{.2in} $\alpha = \left(K/(K-1)\right) \left( 1- 1 \right)$ = 0 \\

\vspace{.25in} %------------------------------

\textit{Claim 3:} Perfect disagreement in items leads to $\alpha = -\infty$ \\

\textit{Proof:} Let $K=2$ and $Y_1 = -Y_2$, thus having perfect disagreement.\\

$\Rightarrow$  $\V{Y_1 + Y_2}$ = $\V{Y_1 - Y_1}$ = $\V{0}$ = $0$

$\Rightarrow$  $\alpha = \left(K/(K-1)\right) \left( 1- \sum_{i=1}^K \V{Y_i} \right.\left/  \V{\sum_{j=1}^K Y_j}  \right)$ = $(2/1)(1-2\sigma^2_y/0)$ = $-\infty$ \\

\end{document}